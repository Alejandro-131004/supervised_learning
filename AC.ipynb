{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "# Supervised Learning\n",
    "\n",
    "Work assembled by Alejandro Gon√ßalves, Pedro Fernandes, Francisca Mihalache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "\n",
    "2. [Chosen Algorithm](#chosen-algorithm)\n",
    "\n",
    "3. [Metrics](#metrics)\n",
    "            \n",
    "4. [Distances](#distances)\n",
    "   - 4.1. [Euclidian Distance](#euclidian-distance)\n",
    "   - 4.2. [L2 Distance](#l2-distance)\n",
    "   - 4.3. [Euclidian Distance (features)](#euclidian-distance-features)\n",
    "   - 4.4. [Cosine Similarity](#cosine-similarity)\n",
    "   - 4.5. [Manhattan Distance](#manhattan-distance)\n",
    "   - 4.6. [Jaccard Distance](#jaccard-distance)\n",
    "   - 4.7. [Mahalanobis Distace](#mahalanobis-distance)\n",
    "5. [Base](#base)\n",
    "\n",
    "6. [Improved Algorithm](#improved-algorithm)\n",
    "   - 6.1. [Bagging](#Bagging)\n",
    "   - 6.2. [KNN-Features](#knn-features)\n",
    "7. [Training](#training)\n",
    "   - 7.1. [Base Training](#base-training)\n",
    "   - 7.2. [Bagging Training](#bagging-training)\n",
    "   - 7.3. [Features Training](#features-training)\n",
    "\n",
    "8. [Comparisons](#comparisons) \n",
    "   - 8.1. [Find k knn](#optimal-number-of-neighborsk)\n",
    "   - 8.2. [Find k Knn comparison](#comparisons-between-models)\n",
    "\n",
    "9. [Statistics](#statistics)\n",
    "   - 9.1 [Confusion Matrix](#confusion-matrix)\n",
    "   - 9.2 [PCA](#pca)\n",
    "   - 9.3 [ROC curve](#roc-curve)\n",
    "   - 9.4 [Statistical Performance](#statistical-performance)\n",
    "\n",
    "10. [Dataset tests](#dataset-tests)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment we'll dive into a selected Machine Learning (ML) algorithm, understanding its theory and testing its performance. We'll explore benchmarking methodologies and differentiate between ML research and practical application, ensuring a balanced understanding of theory and practice in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "To begin with, we need to import some modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score, GridSearchCV, \n",
    "                                     RandomizedSearchCV, cross_validate, LeaveOneOut)\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (classification_report, accuracy_score, precision_score, recall_score,\n",
    "                             confusion_matrix, mean_absolute_error, mean_squared_error, r2_score,\n",
    "                             f1_score, ConfusionMatrixDisplay, roc_curve, roc_auc_score, auc)\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import openml as oml\n",
    "\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Clump_Thickness</th>\n",
       "      <th>Cell_Size_Uniformity</th>\n",
       "      <th>Cell_Shape_Uniformity</th>\n",
       "      <th>Marginal_Adhesion</th>\n",
       "      <th>Single_Epi_Cell_Size</th>\n",
       "      <th>Bare_Nuclei</th>\n",
       "      <th>Bland_Chromatin</th>\n",
       "      <th>Normal_Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Clump_Thickness  Cell_Size_Uniformity  Cell_Shape_Uniformity   \n",
       "0   1                5                     1                      1  \\\n",
       "1   2                5                     4                      4   \n",
       "2   3                3                     1                      1   \n",
       "3   4                6                     8                      8   \n",
       "4   5                4                     1                      1   \n",
       "\n",
       "   Marginal_Adhesion  Single_Epi_Cell_Size Bare_Nuclei  Bland_Chromatin   \n",
       "0                  1                     2           1                3  \\\n",
       "1                  5                     7          10                3   \n",
       "2                  1                     2           2                3   \n",
       "3                  1                     3           4                3   \n",
       "4                  3                     2           1                3   \n",
       "\n",
       "   Normal_Nucleoli  Mitoses   Class  \n",
       "0                1        1  benign  \n",
       "1                2        1  benign  \n",
       "2                1        1  benign  \n",
       "3                7        1  benign  \n",
       "4                1        1  benign  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('csvs/breast.csv')\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                       0\n",
      "Clump_Thickness          0\n",
      "Cell_Size_Uniformity     0\n",
      "Cell_Shape_Uniformity    0\n",
      "Marginal_Adhesion        0\n",
      "Single_Epi_Cell_Size     0\n",
      "Bare_Nuclei              0\n",
      "Bland_Chromatin          0\n",
      "Normal_Nucleoli          0\n",
      "Mitoses                  0\n",
      "Class                    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# If exists any missing values\n",
    "print(df1.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Clump_Thickness</th>\n",
       "      <th>Cell_Size_Uniformity</th>\n",
       "      <th>Cell_Shape_Uniformity</th>\n",
       "      <th>Marginal_Adhesion</th>\n",
       "      <th>Single_Epi_Cell_Size</th>\n",
       "      <th>Bare_Nuclei</th>\n",
       "      <th>Bland_Chromatin</th>\n",
       "      <th>Normal_Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Clump_Thickness  Cell_Size_Uniformity  Cell_Shape_Uniformity   \n",
       "0   1                5                     1                      1  \\\n",
       "1   2                5                     4                      4   \n",
       "2   3                3                     1                      1   \n",
       "3   4                6                     8                      8   \n",
       "4   5                4                     1                      1   \n",
       "\n",
       "   Marginal_Adhesion  Single_Epi_Cell_Size  Bare_Nuclei  Bland_Chromatin   \n",
       "0                  1                     2          1.0                3  \\\n",
       "1                  5                     7         10.0                3   \n",
       "2                  1                     2          2.0                3   \n",
       "3                  1                     3          4.0                3   \n",
       "4                  3                     2          1.0                3   \n",
       "\n",
       "   Normal_Nucleoli  Mitoses  Class  \n",
       "0                1        1      0  \n",
       "1                2        1      0  \n",
       "2                1        1      0  \n",
       "3                7        1      0  \n",
       "4                1        1      0  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replacing benign with 0 and malignant with 1 to be easier to understand and removing the lines that \"?\" is present. \n",
    "df1['Class'] = df1['Class'].replace({'benign': 0, 'malignant': 1})\n",
    "df1['Bare_Nuclei'].replace('?', np.nan, inplace=True)\n",
    "df1['Bare_Nuclei'] = pd.to_numeric(df1['Bare_Nuclei'], errors='coerce')\n",
    "df1.dropna(subset=['Bare_Nuclei'], inplace=True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Clump_Thickness</th>\n",
       "      <th>Cell_Size_Uniformity</th>\n",
       "      <th>Cell_Shape_Uniformity</th>\n",
       "      <th>Marginal_Adhesion</th>\n",
       "      <th>Single_Epi_Cell_Size</th>\n",
       "      <th>Bare_Nuclei</th>\n",
       "      <th>Bland_Chromatin</th>\n",
       "      <th>Normal_Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>352.355783</td>\n",
       "      <td>4.442167</td>\n",
       "      <td>3.150805</td>\n",
       "      <td>3.215227</td>\n",
       "      <td>2.830161</td>\n",
       "      <td>3.234261</td>\n",
       "      <td>3.544656</td>\n",
       "      <td>3.445095</td>\n",
       "      <td>2.869693</td>\n",
       "      <td>1.603221</td>\n",
       "      <td>0.349927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>202.563927</td>\n",
       "      <td>2.820761</td>\n",
       "      <td>3.065145</td>\n",
       "      <td>2.988581</td>\n",
       "      <td>2.864562</td>\n",
       "      <td>2.223085</td>\n",
       "      <td>3.643857</td>\n",
       "      <td>2.449697</td>\n",
       "      <td>3.052666</td>\n",
       "      <td>1.732674</td>\n",
       "      <td>0.477296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>177.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>356.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>527.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>699.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  Clump_Thickness  Cell_Size_Uniformity   \n",
       "count  683.000000       683.000000            683.000000  \\\n",
       "mean   352.355783         4.442167              3.150805   \n",
       "std    202.563927         2.820761              3.065145   \n",
       "min      1.000000         1.000000              1.000000   \n",
       "25%    177.500000         2.000000              1.000000   \n",
       "50%    356.000000         4.000000              1.000000   \n",
       "75%    527.500000         6.000000              5.000000   \n",
       "max    699.000000        10.000000             10.000000   \n",
       "\n",
       "       Cell_Shape_Uniformity  Marginal_Adhesion  Single_Epi_Cell_Size   \n",
       "count             683.000000         683.000000            683.000000  \\\n",
       "mean                3.215227           2.830161              3.234261   \n",
       "std                 2.988581           2.864562              2.223085   \n",
       "min                 1.000000           1.000000              1.000000   \n",
       "25%                 1.000000           1.000000              2.000000   \n",
       "50%                 1.000000           1.000000              2.000000   \n",
       "75%                 5.000000           4.000000              4.000000   \n",
       "max                10.000000          10.000000             10.000000   \n",
       "\n",
       "       Bare_Nuclei  Bland_Chromatin  Normal_Nucleoli     Mitoses       Class  \n",
       "count   683.000000       683.000000       683.000000  683.000000  683.000000  \n",
       "mean      3.544656         3.445095         2.869693    1.603221    0.349927  \n",
       "std       3.643857         2.449697         3.052666    1.732674    0.477296  \n",
       "min       1.000000         1.000000         1.000000    1.000000    0.000000  \n",
       "25%       1.000000         2.000000         1.000000    1.000000    0.000000  \n",
       "50%       1.000000         3.000000         1.000000    1.000000    0.000000  \n",
       "75%       6.000000         5.000000         4.000000    1.000000    1.000000  \n",
       "max      10.000000        10.000000        10.000000   10.000000    1.000000  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols1 = ['Clump_Thickness','Cell_Size_Uniformity']\n",
    "X1 = df1[cols1]\n",
    "y1 = df1['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Clump_Thickness', ylabel='Cell_Size_Uniformity'>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAGxCAYAAACXwjeMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABsDUlEQVR4nO3deXgU9eE/8Pfs7L3ZI5s7EO77RjkEFEVQpIriVa9arfX4tSgq6rdgPevdqrX0a7X6bfGoZyugYrUiIIIcghBAQQEBOULu7H3PzO8PSiQmG7JLsrND3q/n2eeR+Wyy73GzO++d+cysoCiKAiIiIiKN0qkdgIiIiOh4sMwQERGRprHMEBERkaaxzBAREZGmscwQERGRprHMEBERkaaxzBAREZGmscwQERGRpunVDtDRZFlGRUUF7HY7BEFQOw4RERG1gaIo8Pv9KC0thU7X+r6XE77MVFRUoKysTO0YRERElIb9+/eja9eurd7nhC8zdrsdwOH/GQ6HQ+U0RERE1BY+nw9lZWWN2/HWnPBl5sihJYfDwTJDRESkMW2ZIsIJwERERKRpLDNERESkaSwzREREpGkn/JyZtpIkCfF4XO0YHcJgMEAURbVjEBERdYhOX2YURUFlZSU8Ho/aUTqUy+VCcXExr7VDREQnnE5fZo4UmcLCQlit1hNuY68oCkKhEKqrqwEAJSUlKiciIiJqX526zEiS1Fhk8vLy1I7TYSwWCwCguroahYWFPOREREQnlE49AfjIHBmr1apyko53ZB1P1HlBRETUeXXqMnPEiXZoqSWdYR2JiKhzUrXMfPbZZ5g+fTpKS0shCAIWLVrUZFxRFNx3330oKSmBxWLBlClTsHPnTnXCUlaLeWoQqzuIWP0hJMIhteOkLRHwIFZ/CLH6Q5AiQbXjpC3uq0OsrgLxugrE/XVqx0lbItCAWH3F4XXx1qgdp9OTIqH/Ph8HEW+oVDtO2hKJGGJ1FY3vWVoWrzt0+PmoU3c9VJ0zEwwGMXz4cFx33XW46KKLmo3//ve/x7x58/Dyyy+jZ8+euPfeezF16lRs27YNZrNZhcRtJwgCFi5ciBkzZqgd5YSWCPkQrz2A+mWvInpwBwSDGfZhZ8A5djoMucVqx2szOR7973r8A+G9WwFRRM6gCcg99VIY3NqZtB2LxQDPIdSveAOhnV8CAKz9RsF9+pUwFmjrC19jdQfRsOqfCG5fA0gSLD2Hwn3m1RBdRdCbbWrH63TiDVXwbfoY/o0fQ46GYCzqCfekq6DP6wqjq0DteG0Wb6hCYNsq+NZ/ACnohcFditzTL4eptC8MrkK147VZzFOF8Hfl8KxeAMlXC9GRD9f4i2DtNQKG3KKM51F1z8y0adPw8MMP48ILL2w2pigKnnnmGdxzzz244IILMGzYMLzyyiuoqKhotgdHDZWVlbjlllvQq1cvmEwmlJWVYfr06Vi6dKna0TqVeN1BHPrH/Yge3AEAUOIR+L78CFX/+j3i9dr55BZvqETFK/cgvHcLAAWQEghsXYFDrz+oqfWArwYVr96H0I71gCIDiozQt1+g4tV7EKs7qHa6NovVH8Kh13+H4FcrASkBQEF4zxZUvPxbSL5ateN1OvGGStQsfhbeNYsgRw/veY1V7UHlmw8jXr1X3XApiHtqUL/iDTR8+jqkoPfwsvoKVC98GuHvNkGKx1RO2DYJfwP8G5eg7qMXGl8Pkq8WdR+9AF/5EiQCDRnPlLVzZvbs2YPKykpMmTKlcZnT6cTYsWOxZs0aFZMBe/fuxcknn4xly5bhD3/4A7Zu3YqPPvoIkyZNwsyZM1XN1pnEvTWoX/rq4Y3mj8Sqv0es7oAKqVKXCAfgWfUvKInmb2QJbw1Ce7eokCp1UiQEf/knkCOBZmNyOAD/luWQYhEVkqUuvLu8xdKiJGLwrFmoypt1Z5YINCCy7+sWx+qXvoJYgzYO1cixEIJfr2xxrH7FG5opynI0CO8X77c45l33PmQVDpFnbZmprDz8abSoqOnuqqKiosaxlkSjUfh8via39vbrX/8agiDgiy++wMUXX4x+/fph8ODBmD17NtauXdviz/zmN79Bv379YLVa0atXL9x7771NzizavHkzJk2aBLvdDofDgZNPPhkbNmwAAHz//feYPn06cnNzYbPZMHjwYPz73/9u9/XSGiURR/Tgt0nHQ7s2ZjBN+pRIAOHvv0o6Ht71JaRYNIOJ0iOFfQjv2Zx0PLx7M+SgJ3OB0pQI+RDa9WXS8cjerY17BygzIvu2Jx2L11dA0cgejXjN/qRjctivSglIhxT0/nePZUuDCUghb2YD4QS8zsxjjz2GBx98sMN+f319PT766CM88sgjsNmaHzd3uVwt/pzdbsdLL72E0tJSbN26FTfccAPsdjv+53/+BwBw1VVXYeTIkXjuuecgiiLKy8thMBgAADNnzkQsFsNnn30Gm82Gbdu2IScnp8PWUTMEAYLRDCXJp33R5shwoDQJOuhMNsihlou3zpwDQQPXBtKJeuhamUuiM9sgiIYMJkqPTm88xnrkAELWfg48IYlWe/JBnQhBl/2vDwCt/l0BgKDP/tcHAAh643GNd4SsfUUWFx+evFlVVdVkeVVVVeNYS+bOnQuv19t4278/eRNOx65du6AoCgYMGJDSz91zzz0YP348evTogenTp+POO+/E22+/3Ti+b98+TJkyBQMGDEDfvn1x6aWXYvjw4Y1jEyZMwNChQ9GrVy+cd955mDhxYruulxaJNhfsw89MOm4bMC6DadInOvLhOHlq0nH7yLOgE7P/c4fekQ/HqGlJx52jpkHvyP6LU+qMZjhGnp103HHS2TBqaFL2icBcNhBIUlhs/cdCMGnjWmF6VyF0SbKauvaHYNTGeugsOdC7Wp7kq3cVQTRl/sN21paZnj17ori4uMmEWp/Ph3Xr1mHcuOQbKZPJBIfD0eTWnhRFSevn3nrrLUyYMAHFxcXIycnBPffcg3379jWOz549G9dffz2mTJmCxx9/HN99913j2KxZs/Dwww9jwoQJuP/++7FlizbmUHQ00WSBc/R5MBb3bDaWN/V66Cza2DOj0+lgG3AKzD2HNRtzjrtQU2c4mIp7wzbo1GbLbUNOh7GoR+YDpUnvLIBz7PRmyy09h8Pad5QKiTo3wWRF4QWzmu0RM7hLkHvGlTDY3SolS43OnofCi+9qtudCzMlFwU/+H4y52nitG90lKJxxe7NipjNZUXjhbBjyMl/2BSXdrXM7CAQC2LVrFwBg5MiRePrppzFp0iS43W5069YNTzzxBB5//PEmp2Zv2bIlpVOzfT4fnE4nvF5vs2ITiUSwZ88e9OzZs82/r76+Hvn5+XjkkUcwd+7cpPc7+tTsNWvW4LTTTsODDz6IqVOnwul04s0338RTTz3V5Asud+zYgQ8++AAffvghVqxYgTfffLPxTK/9+/fjgw8+wMcff4zFixfjqaeewi233NKmzOmuq1bEGqqQqK9AaNdGiFYHrP3HQGdxwGDPVTtaSuLeGiQ81Qh+uw6C0YycAadAzMmFPkdj69FQBSnoQfDbdQAA24BToLM6YVThdM3jEWuoghIJIPDNWiiJGGz9xkDvyNPUKf8nkrivDko0hODODZD89bD0GAJDQTfN7SVLREKQA/UI796MWP0hWLr2h6m0j6YuwQAA8XgYiq8ekX3bEa3aC1NxD5i7DoTgLGycInG8Wtt+N6OoaPny5QqAZrdrrrlGURRFkWVZuffee5WioiLFZDIpkydPVr799tuUHsPr9SoAFK/X22wsHA4r27ZtU8LhcEq/85xzzlG6dOmiBAKBZmMNDQ2KoigKAGXhwoWKoijKk08+qfTq1avJ/X75y18qTqcz6WNcfvnlyvTp01scmzNnjjJ06NCUMqe7rkRERGpobfv9Y6oeZjrjjDOgKEqz20svvQTg8N6N3/3ud6isrEQkEsEnn3yCfv36qRkZAPDss89CkiSMGTMG77zzDnbu3Int27dj3rx5LR4C69u3L/bt24c333wT3333HebNm4eFCxc2jofDYdx888349NNP8f333+Pzzz/H+vXrMXDgQADAbbfdhv/85z/Ys2cPNm7ciOXLlzeOERERdXbZP6swC/Xq1QsbN27EI488gjvuuAOHDh1CQUEBTj75ZDz33HPN7n/++efj9ttvx80334xoNIpzzz0X9957Lx544AEAgCiKqKurw89//nNUVVUhPz8fF110UeNZWZIkYebMmThw4AAcDgfOOecc/PGPf8zkKhMREWUtVefMZEJ7z5nRqs60rkREpH2pzJnJ2rOZiIiIiNqCZYaIiIg0jWWGiIiINI1lhoiIiDSNZYaIiIg0jWWGiIiINI1lhoiIiDSNZYaIiIg0jWWGiIiINI1lRsOeffZZ9OjRA2azGWPHjsUXX3yhdiQiIqKMY5nRqLfeeguzZ8/G/fffj40bN2L48OGYOnUqqqur1Y5GRESUUSwz7UCSFWzdVYsVGw9g665aSHLHf93V008/jRtuuAG/+MUvMGjQIDz//POwWq34+9//3uGPTURElE34rdnHafWWCrywaCvqvJHGZXlOM26cMRTjh5V2yGPGYjF8+eWXmDt3buMynU6HKVOmYM2aNR3ymERERNmKe2aOw+otFXjs5fVNigwA1HkjeOzl9Vi9paJDHre2thaSJKGoqKjJ8qKiIlRWVnbIYxIREWUrlpk0SbKCFxZtbfU+L777VUYOOREREXVmLDNp2ra7rtkemR+r9YSxbXdduz92fn4+RFFEVVVVk+VVVVUoLi5u98cjIiLKZiwzaar3tV5kUr1fKoxGI04++WQsXbq0cZksy1i6dCnGjRvX7o9HRESUzTgBOE1uh7ld75eq2bNn45prrsGoUaMwZswYPPPMMwgGg/jFL37RIY9HRESUrVhm0jSoVx7ynOZWDzXluywY1CuvQx7/sssuQ01NDe677z5UVlZixIgR+Oijj5pNCiYiIjrR8TBTmkSdgBtnDG31PjdcMASiTuiwDDfffDO+//57RKNRrFu3DmPHju2wxyIiIspWLDPHYfywUsy9ZjTynE0PJeW7LJh7zegOu84MERER/YCHmY7T+GGlGDukBNt216HeF4HbYcagXnkdukeGiIiIfsAy0w5EnYChffLVjkFERNQp8TATERERaRrLDBFRK2RZRkKS1Y5x3BRFhiIl1I5x3BRFgcz1yCpSLKp2BB5mIiJqiTcQxYHqAD5cvQfRuIwpY8rQp6sLeU6L2tFSIoX9iDdUwf/lfyBF/LANnABLt0HQOzrmshEdRYqEkPBWw7dpCSRfLax9R8HSawQMzgK1o6VEjkeR8NbAv3kZ4nUHYe42GLYBY6F3FkAQtLN/QY5FkfBWw791BeI1+2Ao7A770InQOwqhM5oynodlhojoRzyBKOa//zWWbdjfuGztV4fQt6sTv71urGYKjRQOwLv2PXhWL2hcFtqxHvrcYpRc9YBmioAcCyO4bSVqP3yhcVlo5waINhdKf/4wDO4SFdO1nSzFEd5djqp3ngSUw3v7Qjs3oGHVP1F69UMwFfVQN2AbyXIC0YPf4tBbjwBH9i7t+hK+de+j+LK7Yeo+BDpdZouZdmogEVGG7K/0NykyR+w84MXK8oOQNfIFsglfbZMi07i8oRLete9CTsRVSJU6KeBB7YcvNl8e9KB2yXxIkZAKqVIn+RtQveiZxiJzhBINoeb9PyMR9KoTLEWSpxbV7/3phyLzX4oUR/V78yB5qzOeiWWGiOgoiYSMf6/ek3T835/vhTeg/hyBtghsW5V0zL95GeSQL4Np0hfetw1AywUyvGsj5Ig/s4HSFK8/BCURa3EsVrUXclgb6yGFfZACnpbHAg2QVPi7YpkhIjqKrCiIxKSk49G4BFnRxp4ZJZb861aURByKVtYj3lp5VABZGxO0kxWZxnE5+d9dNlGk1nOqsR4sMxr12WefYfr06SgtLYUgCFi0aJHakYhOCEaDiDNHdU06PmFYCRw2YwYTpc82YFzSMUufk6AzWzOYJn3mboOTjhmLekJnsmUwTfqM+V2BJJN8RbsboiUnw4nSI9qcEAwtT/IVDGaIVmeGE7HMaFYwGMTw4cPx7LPPqh2F6IQzqEceupfYmy132Iw4f2JvGPSiCqlSZ8jrAnO3Ic2WCwYT8ib9DKJJG2VGtOfCNuS05gM6Efnn3ADR5sh8qDSINidc42a0MCIg/5wbIea4Mx0pLWKOG7lnXNnimHvSlRAdmV8Pns3UDhRZQmT/dkiBBog5uTCXDYSg69g3u2nTpmHatGkd+hhEnVWey4IHrh+Hpev34aO1exFPyJgwrBQzTu+DIrc2CgAA6HNcKJxxK4LfrIH3iw8gR0Ow9h4J16mXwJBbrHa8NtNbHcibfC2sPYfDs3ohpJAXprKBcE+8HIY87XwHns5khXPsdJhK+6Bh5dtIeGthLO4F9xlXwFhQBkHQxtfgiCYzcgaOh8FdCs/KtxGvPwRDXilcp14KU1EPiAbzsX9JO2OZOU7Bb9ai9uO/Q/LXNS4T7XnIP/s62AacomIyIjoe+S4LLjmzL84a0x0KFNitRhgN2tgjczS93Q3HqJ/ANnACoEjQmXOgS3KIIJvpc1ywD5sES6+Rh9fDaIXOpI1T5I8mWh2w9R8Lc9cBUKQEBKMZolkbh8mOpre7obe7YSzqAcSjgMEIg129axexzByH4DdrUfXOH5otl/x1qHrnDyi6+C4WGiINE0Ud3M7Mf8psb4IgQJ/jUjtGuzhR1kO0ZX5eSUcw2LPj0BjnzKRJkSXUfvz3Vu9Tu+TvmpmdTkREpFUsM2mK7N/e5NBSSyRfHSL7t2coERERUefEMpMmKdDQrvcjIiKi9HDOTJrEnNx2vV+qAoEAdu3a1fjvPXv2oLy8HG63G926deuQxyQiIspGLDNpMpcNhGjPa/VQk+jIg7lsYIc8/oYNGzBp0qTGf8+ePRsAcM011+Cll17qkMckIiLKRiwzaRJ0IvLPvq7Fs5mOyD/rug673swZZ5yhmUuRExERdSTOmTkOtgGnoOjiuyD+6Nx60ZHH07KJiIgyhHtmjpNtwCmw9hud8SsAExER0WEsM+1A0ImwdG/+/SdERETU8XiYiYiIiDSNZYaIiIg0jWUG6BRnBXWGdSQios6pU5cZg8EAAAiFQion6XhH1vHIOhMREZ0oOvUEYFEU4XK5UF1dDQCwWq0QBEHlVO1LURSEQiFUV1fD5XJBFHmWFRERnVg6dZkBgOLiYgBoLDQnKpfL1biuREREJ5JOX2YEQUBJSQkKCwsRj8fVjtMhDAYD98gQEdEJq9OXmSNEUeQGn4iISIM69QRgIiIi0j6WGSIiItI0lhkiIiLSNJYZIiIi0jSWGSIiItI0lhkiIiLSNJYZIiIi0jSWGSIiItI0lhkiIiLSNJYZIiIi0rSs/joDSZLwwAMP4B//+AcqKytRWlqKa6+9Fvfcc88J9+3WRIlAA+RICAlfLQRRhJiTC53NBb3Zpna0lMRDfihhHxK+OgiCANHuBsx2GG0OtaOlJBGNQgrUQQ40QJES0DvyoVgcMNnsakdLSSIRg+ythRT0QIlHoXfmQzBaYXDkqR0tZbH6Q5BDPsiRIERHPnRmmybXI+6phhTyQQ75IDryoDPnaHI9Yg1VUCIBSEEPxJxcCCYbjLlFqmTJ6jLzxBNP4LnnnsPLL7+MwYMHY8OGDfjFL34Bp9OJWbNmqR2PqN3EPVXwb/0MnlX/AuQEAEBntqFg+s1QuvSHweZUOWHbxL3VCO/ejLqP/w4lEQMACAYT8s7+JYSew2Fw5qucsG2ioQDi+79G7fv/CzkaOrxQ1CP3tJ8CQ86AyamNDU8iEkK8ajeqFjwFOeQ7vFDQwTFqGpxjzoPBVahuwDaSJAmJ6r2o+tcfkPDV/HepANvgU+E+40rNrAcAxGoPoOqdPyBee6BxmaXXCORPu0lb61FfgZp35yFasbNxmam0LwouuBVGd0nG82T1YabVq1fjggsuwLnnnosePXrgkksuwdlnn40vvvhC7WhE7SpauReez95sLDIAIEeCqHrnSShBj3rBUiT56lD77+cbiwwAKPEoaj/4CyR/nYrJUuSvQ/U7T/5QZABASqDh09eRqN6jXq4UyYF6VL75yA9FBgAUGb71HyD03Ub1gqVI8lTh0BsPHVVkAEBB8OuV8G38D6RYVLVsqYg3VKHy7cebFBkACO8uR/2nr0EK+1VKlpp4QxVq3n+2SZEBgGjFTtQu/gvinuqMZ8rqMjN+/HgsXboUO3bsAABs3rwZq1atwrRp05L+TDQahc/na3Ijymaxhip41yxoeVCW4N+8HJIkZTZUGhIBDzzr3ks67l33HhIBbwYTpUdKJOAv/wRQ5BbHPZ8vQNRTm+FU6Qnt2tSkWB7Nu2YRYvUVGU6Unlj195CTbOh9Gz/WTFFOBOqRaDjU4lhw22pIwex/fQCAHAsjeuCbFsci+7c1/RCQIVl9mGnOnDnw+XwYMGAARFGEJEl45JFHcNVVVyX9mcceewwPPvhgBlMSHSdZQryhKulwvL4CSjwCiNk9d0aORZBobT08VZDjYQDZfchMjkUQb2Ujn/BUAVI8g4nSF6vdn3Qs4a2BoGQwzHGI1yV/PpRoCIpGno+Er5USrMiQY+HMhTkOciR4XOMdIav3zLz99tt47bXX8Prrr2Pjxo14+eWX8eSTT+Lll19O+jNz586F1+ttvO3fn/zFTJQNBL0BxvyypOPG4p6amASsM1lhKGhlPQq6Q2fSwnqYYSzqmXTcUFAGGMwZTJQ+U0nvpGOGvC6ARk6kMBZ1TzqmszogiIYMpkmfwVWcdEwQDdCZrBlMkz6dJee4xjtCVpeZu+66C3PmzMHll1+OoUOH4uqrr8btt9+Oxx57LOnPmEwmOByOJjeibGZwFsB12qUtjgkGE3IGn5bhROnR2xxwjT0fEFp4WxF0cI45D3pr9p8JJIp65Aw9HYLe2OJ47qmXwuTIzXCq9Fh6DIHO3PKGJfe0n8KgwkTNdBjyukDvKGhxzHXK+RCdLY9lGzHHBWNxrxbHckZMhpjjznCi9OhMVlj7jmpxzNp3FHQqfPjK6jITCoWg0zWNKIoiZLnlY9lEWmVwlyL/vJlNPtHoXUUovvwe6BzaOAMIAHQ5uSi65C6IOa7GZaLdjaJLfwOdRt6oAUB0FKLo8nugd/5wdonOYkfB9Fugyy1VMVlqdM4ilFx5Hwz5XRuXCUYL3JOvgalLPxWTpcboLkHx5b+FqbRP4zJBb4Rz3IWwDZwAUa+RPTPOAhReOBvmHkN/WKgTYR8xBa5TZkA0WdQLlwKDswB5Z10Ha/+xP3x4EXSwDjgFeWf9AgYV3rMERVGy9qjptddei08++QR//etfMXjwYGzatAk33ngjrrvuOjzxxBNt+h0+nw9OpxNer5d7aSirJWIRyP56yJEAoBMhmqya+eR8tHg8DsVXc3g9BAGiOQew58Ng0MYG52jR+kookQAURYbObIfoyIdeg+sRb6iEHDk8t0RnyYGY49bMhvNocU815EgQSiIGnTkHOpsTehUOaRyvhL8ecjgAORbW9np4ayHHQpCjYehMFuhMNujb8Xo5qWy/s7rM+P1+3HvvvVi4cCGqq6tRWlqKK664Avfddx+MxpZ3Af8YywwREZH2nDBlpj2wzBAREWlPKtvvrJ4zQ0RERHQsLDNERESkaSwzREREpGksM0RERKRpLDNERESkaSwzREREpGksM0RERKRpLDNERESkaSwzREREpGksM0RERKRpLDNERESkaSwzREREpGksM0RERKRpLDNERESkaSwzREREpGksM0RERKRpLDNERESkaSwzREREpGlplZnTTz8dr7zyCsLhcHvnISIiIkpJWmVm5MiRuPPOO1FcXIwbbrgBa9eube9cRERERG2SVpl55plnUFFRgfnz56O6uhoTJ07EoEGD8OSTT6Kqqqq9MxIREREllfacGb1ej4suugjvvvsuDhw4gCuvvBL33nsvysrKMGPGDCxbtqw9cxIRERG16LgnAH/xxRe4//778dRTT6GwsBBz585Ffn4+zjvvPNx5553tkZGIiIgoKUFRFCXVH6qursarr76K+fPnY+fOnZg+fTquv/56TJ06FYIgAABWrVqFc845B4FAoN1Dp8Ln88HpdMLr9cLhcKiahYiIiNomle23Pp0H6Nq1K3r37o3rrrsO1157LQoKCprdZ9iwYRg9enQ6v56IiIiozdIqM0uXLsVpp53W6n0cDgeWL1+eVigiIiKitkprzsz9998Pj8fTbLnP58OZZ555vJkog6SgF3FPDRK+OiiSpHactEkhP+LeGiR8tZATcbXjpE2KBBH31iDuq4Ucj6odJ20xXz1idRWHb/56tePQCUKKhBCrr0Cs7iDiDZVqx0mboshI+OoOv/cGPGrHOS7x+kOHn4/6Q6rmSGvPzIoVKxCLxZotj0QiWLly5XGHoo4nR0OIHvoOdUvmI1b9PXRmGxyjz4Vj5FnQ291qx2szOR5FrHof6pbMR/TgtxAMZthHToFr7PnQO/LUjtdmipRArO4g6j95GeE9WwBRRM6Qicg99VIYXIVqx2uzeDwOuaECDSveQGjnlwAAa79RyD39SpgKylROR1oWb6iCb9PH8G/8GHI0BGNRT7gnXQV9XlcYXc2nOmSrRMCDwFefwbt2EaSgFwZ3KdxnXg1zt0EQLTlqx2uzmKca4e82wbN6ASRfLURHPlzjL4K190kwqPB8pDQBeMuWLQCAESNGYNmyZXC7f9joSZKEjz76CH/961+xd+/edg+aLk4Abllw15eoeuvRZsstvUai8PxbINqcKqRKXaRiJypeuhtQ5CbLjUU9UXzZ3ZopZrHaAzj4t7ugJJp+SNA7C1B69cPQO/NVSpaaWO1BVLx8N+RI04n/OksOSq95FMa8LiolIy2LN1SiZvFfENn3dbOxokvnwNZPG/MzpXAAdZ+8hMCW5lMwCs6biZyhp0PQiSokS00i5Id37bvwrlnYbMw5/kI4x14AvdV+3I/TYROAR4wYAUEQIAhCi4eTLBYL/vznP6eWljIuEWhA3X/+1uJYePcmJPx1migzUtiPuo9falZkACBWtQex2gOaKDNyLIKGVf9sVmQAIOGtQfj7r2Afdkbmg6VIioThL/+kWZEBADkcgH/zcrhOvQSi0axCOtKyRKChxSIDAPVLX4GhoCuMuSUZTpU6KeRtscgAQN3SV2DpMUwTH1zkkBfeL95vccy77n3Yh54BtEOZSUVKZWbPnj1QFAW9evXCF1980eQsJqPRiMLCQohi9rfKzk6OhpDwJL9Sc+TAtzAV98pgovQosQiiB79JOh7auQHWnsMymCg9cjR4+NBSEsFv1sA2aAJ0ekMGU6UuEfYivGdz0vHwns2wj5wC0VicwVR0Iojs2550LF5fASXe/INANorXHkw6Jof9kCIBTZQZKegFpESSwQSkkBdA14xmSqnMdO/eHQAgy80/CZN2CDo9IOha3KMBAKJVI4fjBB0EoxlKLNLisGhzZTZPugQROpMNcsjX4rBocUDQZf8X3OtEPXRmW/Jxsw0QjRlMRCcKsbVP+TpRE4dmAEBntrY6LmT5B5YjBH3rr+NjjXeENpeZ9957D9OmTYPBYMB7773X6n3PP//84w5GHUe0OmDrPxbBb9a0MKiHqbRP5kOlQbQ54ThpKrxr321x3NZ/bIYTpUe0OeEcex7qPnqxxXH7yVM18WZtcOTDMWoaIvu2tTjuGDUNRkf2H/aj7GMuGwjoREBufsalrf9YCKbWS0K20LuKoDNZIUdDzcZMXQdAtGjjg6TOkgO9q6jFPfx6VxFEc+YnMre5zMyYMQOVlZUoLCzEjBkzkt5PEARIGj7FtzPQmSxwT/45olV7kDj69EZBh6KL7oKYk6teuBQIoh7O0ecivPcrxCq/azKWf+6vNDFfBjj8mrH1H4vQjg0I797UZMx12k9hyNXOYRljcW/YBp2K4LZVTZbbhpwOY1EPdUKR5gkmKwovmIXqRX9qskfZ4C5B7hlXwqCR17re7kbRT+ei8o2HmsyRE3NyUTh9Zut7oLKI0V2Cwhm3o/KN3zUpZjqTFYUXzobBnfn5S2l9nYGW8Gym5BK+OsSqv0d471bonQWw9j4Jot0NnUFbhwISgQbE6w4itPNLiDYXrP1GQZ/jhs5kUTtaShJBLxINlQju+AI6gxm2AWMh2vMgtnLoJhvFGqogBz0IfrMOEADbgFOgszphzC1SOxppWNxXByUaQnDnBkj+elh6DIGhoBuMKmw4j4ciJZDw1SG8dwvitQdhKhsIc2lv6B3ZP1fmaJIUh+SpRmTfdkSr9sJU3APmrgMh5ha329zZVLbfLDNERESUdTr8u5kAYP369Vi+fDmqq6ubTQh++umn0/21RERERClJq8w8+uijuOeee9C/f38UFRU1flM2gCb/TURERNTR0iozf/rTn/D3v/8d1157bTvHISIiIkpNWhev0Ol0mDBhQntnISIiIkpZWmXm9ttvx7PPPtveWYiIiIhSltZhpjvvvBPnnnsuevfujUGDBsFgaHrVwgULFrRLOCIiIqJjSavMzJo1C8uXL8ekSZOQl5fHSb9ERESkmrTKzMsvv4x33nkH5557bnvnISIiIkpJWnNm3G43evfu3d5ZiIiIiFKWVpl54IEHcP/99yMUav5lWURERESZlNZhpnnz5uG7775DUVERevTo0WwC8MaNG9slHBEREdGxpFVmWvvWbCIiIqJMSrnMJBIJCIKA6667Dl27du2ITERERERtlvKcGb1ejz/84Q9IJBIdkYeIiIgoJWlNAD7zzDOxYsWK9s5CRERElLK05sxMmzYNc+bMwdatW3HyySfDZrM1GT///PPbJRwRERHRsQiKoiip/pBOl3yHjiAIkCTpuEK1J5/PB6fTCa/XC4fDoXYcIiIiaoNUtt9p7ZmRZTmtYERERETtLa05M0RERETZIu0ys2LFCkyfPh19+vRBnz59cP7552PlypXtmY2IiIjomNIqM//4xz8wZcoUWK1WzJo1C7NmzYLFYsHkyZPx+uuvt3dGIiIioqTSmgA8cOBA3Hjjjbj99tubLH/66afx4osvYvv27e0W8HhxAjAREZH2pLL9TmvPzO7duzF9+vRmy88//3zs2bMnnV9JRERElJa0ykxZWRmWLl3abPknn3yCsrKy4w5FRERE1FZpnZp9xx13YNasWSgvL8f48eMBAJ9//jleeukl/OlPf2rXgEREREStSavM/OpXv0JxcTGeeuopvP322wAOz6N56623cMEFF7RrQCIiIqLWtHkC8Lx583DjjTfCbDZj3759KCsrgyAIHZ3vuHECMBERkfZ0yATg2bNnw+fzAQB69uyJmpqa40t5ApClBNI4GYw6iByPQZaz56s00qXICSgnwFW25VgUciKmdozjJkuJE2I9JCkOORpRO8ZxkyQJUjSkdozjpigKZCmhdox2kQgH1Y7Q9sNMpaWleOedd/CTn/wEiqLgwIEDiERafmF069at3QIePHgQv/nNb/Dhhx8iFAqhT58+mD9/PkaNGtVuj5EKRZaQ8NYi+M1aRPZvgyG/K+zDJkHvLIDOYFIlU2cXr69EeN9XCO1YD505B46RZ0HvKoTe7lY7WkoS/npEK3bCv+VTCEYznCedDYO7FKLNqXa0lMTrKxGr3Y/A1k8BADnDJsGY3xWG3GJVc6Uq7q1FvP4g/OXLoEgx5Aw6FabSPjC4itSOlpK4pxoJfx38m5ZAjoZg7Tsalm6DYHCXqB0tJXFvLaSgB/7yTyAFGmDpMQyW3iNhzCtVO1pK5HgUCW8N/JuXIV53EOZug2EbMBZ6ZwEEQTsX5ZeCfkghD/xbVyBesw+Gwm6wDzkdotUF0WbPeJ42H2Z64YUXcMsttyCRSN4kFUVp1y+abGhowMiRIzFp0iT86le/QkFBAXbu3InevXujd+/ebfod7X2YKVq5BxWv3gMldlSRE3QouuR/YO09EoKY1jQkSlOs/hAq33gICU9Vk+XOcTPgHH2uZgpNwleHyrcfR6xqd5PlOcPPRN6kn2mm0MQbKlHzwXOIfP9Vk+WWnsOQN+0mGDVSaOLeGtQv+weC21Y1WW4s7oWii+/UTKGJe2rg2/ABvOveb7Jcn1uM4svvgVEjhSbmr0No+xrUL5nfZLloc6HkqgdgLNDGWbSyFEd410ZUvfMkoPyw91UwWVF69UMwFfVQL1wK4vEIEgd24NBbjwBH7V0SRAOKL7sb+q4DYTAYjvtxOuQw04033oja2lps3rwZiqJgyZIl2LhxY5Pbpk2bsHHjxuNegSOeeOIJlJWVYf78+RgzZgx69uyJs88+u81Fpr0lAh5Uv/unpkUGABQZ1YueQSJQr0quzkqKhuD5/J1mRQYAvGsWQQp4Mh8qDYoswb91RbMiAwCBzcsQq6tQIVV6Ivu2NSsyABDeswXRAztUSJSeeO2BZkUGAGKVuxH4ehVkWRuHB+SQt1mRAYBEQyW8axchEQ6okCoNkRDql7zUbLEU9KDuk5eR8NVmPlMaJH8Dqhc906TIAIASDaHm/T8jEfSqEyxFiq8e1e/9qUmRAQBFiqP6vXlQ/JmfhpLSPi273Y4hQ4Zg/vz5mDBhAoYPH97irb289957GDVqFC699FIUFhZi5MiRePHFF1v9mWg0Cp/P1+TWXuSwH/Ha/S2OKfEIEp7qdnssOjY56EXw6+YbnCMC2z/PYJr0SSEf/Js+Tjru2/gfTcyhiXtr4Nv0SdJx/6YliHvrMpgoPVI8Cn95K+uxeRkkX/avBwAEWnt9fLUSSqj93h87UnjvVgAtH0QI7y6H/OMPmFkqXn8ISpL5V7GqvZDD/gwnSo8c9if9sCgFGiCHMl+S0zpAd80118Bk6vj5Ibt378Zzzz2Hvn374j//+Q9+9atfYdasWXj55ZeT/sxjjz0Gp9PZeGvPi/gpx5hcqsS1P0lQSxQASisT6JRYNHNhjoeiJH2DAw4XZUXJ/jKjyFLr65GIAlrYoyHLrU74VRIxzUz8l+PJXwNKIp6kHmSf1idgK8d8b84Wrb0+gGNvY7LFMbeFKrzO21xm3G43amsP78rLzc2F2+1OemsvsizjpJNOwqOPPoqRI0fixhtvxA033IDnn38+6c/MnTsXXq+38bZ/f8t7UtIhWuwQba6WBwUdDBqbiKZ1OqMFlp7Dko7bBozNYJr06Sw5sPZPnjVn6BnQaWAulujIg63fmKTj1gGnQLDnZTBRekSTBTkDJyQdt/YZBdHqylyg49Daa8DSa4RmTlqwdh+SdMxY1BOCwZzBNOkz5ncFkkzyFe1uiJacDCdKj2hzQkjytyMYzKrM8WvzO+Qf//hH2O2HZyg/88wzHZWniZKSEgwaNKjJsoEDB+Kdd95J+jMmk6nD9hqJ9lzknXMjqt/5fbMx14SLIVq1MUnzRKHPccF95s9Q8fL2Zp94zN2GQO/SxmRTnd4I1ykXILhtNeRI092zxsIeMHfpq1Ky1IiiAbYhp8JX/gkkf9PDMHpHAWwDxkGvz/5SBgDmsgEwFJQhXtP0w5DOYodz7HkQTdrYeOpzi2EuG4jI/qZf/isYTHCfcSX0juwvlwAgWOywDRyH4PY1TQd0IvLO+gWMudqYkC3anHCNmwHP6gU/GhGQf86NEHO0ccKCYLEj9/QrUP/JS83G3JOuhGjJ4rOZ1HDllVdi//79WLlyZeOy22+/HevWrcPq1avb9Dva+2wmORpGrGYf6j99HbGqvdA7C5B72k9hLhsA0cqL8mWaHI8i3lAJz6p/Ibx3y+FTs086G7YB42FwFagdr80URUHCUwXP2ncR/GYtBL0RjpFnwz7sDM1scI6I1R+Cb8OHCG7/HIAA26AJcJw8FUa3tvZcxhsq4d+8HP4ty6FIcVj7jYFr7PnQu0ug02nnFNp4QxWC36yBb+PHkKMhWHoOQ+6Ei6FzFUFv1EYpAw6vR2hPOXxffAAp5IWpS3+4J14GnSMfBo2c7QccniMX2b8dDSvfRsJbC2NxL7jPuALGgjLojBa147VZ3FONWO1+eFb+E/H6QzDklcJ16iUw5nWFoZ3KZSrb77TLjCzL2LVrF6qrqyH/aHLixIkT0/mVzaxfvx7jx4/Hgw8+iJ/+9Kf44osvcMMNN+CFF17AVVdd1abf0VFXAJYiQSixCAS9gSUmCyTCASjhAKDTQXTka2pjczQ5EfvvJEAdRJsDgk5UO1JapEgIUtADABBz3JrZk/FjshRvnOyrszogmqwqJ0pPIpGA7KsBFAU6kxX6HJfakdIWqz8EKDIEgwUGhzb2ZLRECnqhSAkIRjNEs03tOGmLe6qhSHEIor7dL1nQ4WVm7dq1uPLKK/H99983mwjXnteZAYDFixdj7ty52LlzJ3r27InZs2fjhhtuaPPP8+sMiIiItKfDy8yIESPQr18/PPjggygpKWn2HU1OZ/bs8mOZISIi0p5Utt9pzcbbuXMn/vWvf6FPnz5pBSQiIiJqL2lNLBg7dix27drV3lmIiIiIUpbWnplbbrkFd9xxByorKzF06NBm38EwbFjya38QERERtae05sy0dKaIIAjt/kWT7YFzZoiIiLSnw+fM7NmzJ61gRERERO0trTLTvXv39s5BRERElJaUysy8efNaXO50OtGvXz+MGzeuXUIRERERtVVKZeaPf/xji8s9Hg+8Xi/Gjx+P9957r12/bJKIiIioNSmdmr1nz54Wbw0NDdi1axdkWcY999zTUVmJiIiImmm3L7Dp1asXHn/8cXz88cft9SuJiIiIjqldv42vW7duqKysbM9fSURERNSqdi0zW7du5ZlORERElFEpTQD2+XwtLvd6vfjyyy9xxx134JprrmmXYERERERtkVKZcblczb4h+whBEHD99ddjzpw57RKMiIiIqC1SKjPLly9vcbnD4UDfvn2Rk5PTLqGIiIiI2iqlMnP66aen9Mt//etf43e/+x3y8/NT+jkiIiKitmrXCcA/9o9//CPpPBsiIiKi9tChZSaNL+QmIiIiSkmHlhkiIiKijsYyQ0RERJrGMkNERESaxjJDREREmtahZeZnP/sZHA5HRz4EERERdXJpl5mVK1fiZz/7GcaNG4eDBw8CAF599VWsWrWq8T7PPfccrzFDREREHSqtMvPOO+9g6tSpsFgs2LRpE6LRKIDD39H06KOPtmtAIiIiotakVWYefvhhPP/883jxxRdhMBgal0+YMAEbN25st3BEREREx5LS1xkc8e2332LixInNljudTng8nuPNlPWksB9S0Asp0ACdxQ7R5oI+x6V2rJQlAh5IYR8kXx10ZhtEqwOG3GK1Y6VMjoYghXxI+GohGMzQ57gg2t0QBG3Nb5fCAUghLxLeWgiiHqLdDb3dDZ3BpHa0lMRDAShhLxK+OgiCcPi5MDtgsNnVjpYSOR5Fwl8PyV8PRUpA78yHaHVCtGjrO+ji8SgUXx2koAdKPAq9Mx+C0QKDQ3tTAOINlZBCXsiREERHHkSLHfqcXLVjpSzWUAk57Icc8kG050Ew2WB0FagdK2WxhiookQCkoAdiTu7h9cgtUiVLWmWmuLgYu3btQo8ePZosX7VqFXr16tUeubJWwl+HmsXPIbx7U+MyQ0EZii+ZA4NbO0Ug7q1B/dJXENy+unGZ3lmAokt+A1NxTxWTpUYKetGweiF86z8AFBkAINpcKLr0NzCV9IagE1VO2DZxfx385cvgWfUvQE4AAHRmGwqm3wJzt8EQzVaVE7ZN3FeD8HflqPv471ASMQCAYDAh7+xfAr1GwODIUzlh20iRICLff4Wa9/8XcjR0eKGoR+5pl8E+7Azo7W51A7ZRIhxEonoPqhY8BTn036+WEXRwjJoG55jzYHAVqhuwjWRZRrxqD6r+9QckfDX/XSrANvhUuCddBYNTO0UgVrMfVQueRLz2QOMyS68RyD/nRhhUKgLpiNVXoObdeYhW7GxcZirti4ILboXRXZLxPGl9dL3hhhtw6623Yt26dRAEARUVFXjttddw55134le/+lV7Z8wacjSMuk9eblJkACBesx+Vbz+KRKBBpWSpkeIR+L74oEmRAYCEtwaVbz6EWP0hlZKlRlEUBL9dB98X7zcWGQCQgh4ceu0BJHy1KqZLTbRiFzyfvdlYZABAjgRR9c4fIGloPSRvLWr//XxjkQEAJR5F7Qd/0dR6JHy1qHrnyR+KDABICTR8+hqilbvVC5YiOdiAyjcf+aHIAIAiw7f+A4S+086UAMlThUNvPHRUkQEABcGvV8L35UeQ47GkP5tNYvWHUPnPJ5oUGQAI7y5H/YrXkQh6VUqWmnhDFWref7ZJkQGAaMVO1C7+C+Ke6oxnSqvMzJkzB1deeSUmT56MQCCAiRMn4vrrr8dNN92EW265pb0zZg0p5EFw+5oWx+J1ByH56zOcKD2Srx6+TUtaHgt6Ea/dn+FE6ZECDWhY9c8Wx5R4FOE9WzOcKD1xXy28qxe0PChL8G9eBlmWWx7PIomgF5517yUd9657D4lg9n/xrCwl4N/0SZOCfDTP5wsQ99dlOFV6Qrs2NSmWR/OuWYRYfUWGE6UnWv095LC/xTHfxo+R0MjzIQU9SDS0/GExuG1109KZxeRYGNED37Q4Ftm/remHgAxJ6zCTIAj47W9/i7vuugu7du1CIBDAoEGDkJOjrWPJqZJjkaRvcACQCDRAC7MblEQMSjySdDxep5E9M7LUaoGMVX+fwTTHIZFAvKEq6XC8vuLwBslozmCo1MnRMBKtrYenCnIsDNiy+9pTSjyKeCsb+YSnCkoinsFE6Yu18sEk4a2BoJHvAo7XHUw6pkRDSQtbtkl4W9k7qciHtzEaIEeCxzXeEdLaM/PKK69g+/btMBqNGDRoEMaMGYOcnBxEIhG88sor7Z0xa+iMVkCXvP/pNTKhTjCYoDMnL57Gwm4ZTJM+QdRD38qEZXOXfhlMkz7BYIIxvyzpuLG4J8QsLzLA4Tk+hoJW1qOgO3Sm7J/7IxjNMBYlnzdmKCiDzpD9zwcAmEp6Jx0z5HUBBCGDadJnLOyedExndUDQGzOYJn2tzYkRRAN0JksG06RPd4xJ8Mca7whplZlrr70WY8aMwTvvvNNkudfrxS9+8Yt2CZaNxBwX7CPPanHMVNoPokZm1eud+XCOnd7ymKsIBndphhOlR5+TC/ekq1oc01nsMJUNyHCi9OjtuXCd9tMWxwSDCTmDm585mI30VjtcY88HWjqLTNDBOeY86K3Zf0aTTifCPuyMpBvI3FMv1czZi5YeQ5J+cMk97acwqDBRMx3G/K7QO1qe5Os65QLoNTIBWLQ6YSxu+SSZnBGTobNpYxuiM1lh7TuqxTFr31HQmW0ZTnQcVwB+8MEHcfXVV+OBBx5oxzjZTWcwIffUi2EfeTZw1Fkylp4jUHjRbOhtThXTtZ1ONCBn6BlwnnIBBPGH6wSZSvui+LK7NTWj3tJjKPLOvg7CUXsuDAVlKPnZ7zR1hoOxoAwF593c5BON3lWE4svvgV4jZ5wAgM7uRtEld0E8amMv2t0ouvQ30Nm1cSYTcPjMvuLL74He+cP/e53FjoLpt8CQ31XFZKkRHIUoufK+JpkFowXuydfA1KWvislSY8gtRvEV98BU2qdxmaA3wjnuQtgGnQqdmNaMiYwz5Bah8MLbYe4x9IeFOhH2EVPgGnsB9JbMl4B0GJwFyDvrOlj7j/3hw4ugg3XAKcg76xeqnPYvKIqS8lFTnU6HyspK7N69GxdeeCEmTJiAV199FT6fD6WlpZAkqSOypsXn88HpdMLr9bbb90TJsQikoAdyJATBaIZoc0Bs5bBNtpKiIUiBBsjhAASDCaIlRzOHyo6mSAkkAg2Qw/7Du2qtduhtLrVjpUxOxCH5aiGF/RB0InSWHBhc2imWRyQSCcjeasiRACAIEM05EBwF0Ou1scE5WryhClLED8gyRIsdoqMAOk2uRyXkSAiKFIfOkgPRlquZ0/2PlvDWQooEoMSjh6/xleOCqIFDlz8W99ZAiYYgxyLQmW3QWRzQZ/lcspYkvLWQYyHI0TB0Jgt0Jhv07Xj5hVS232mVGVEUcejQIRQWFmLfvn04//zzIQgCnn/+eYwfP/6ELzNERETUsVLZfqd1mOno/tOtWzesXr0aPXr0wFlntTyfhIiIiKijpFVm7r///ianYVutVixcuBC33357i19zQERERNRR0jrMpCU8zERERKQ9qWy/2zyT7b333sO0adNgMBjw3nvJr/IpCAKmT2/5tF8iIiKi9tbmPTNHzmAqLCyETpf86JQgCJwATERERMelQ/bMHP3dMFr4nhgiIiLqHNK+aB4RERFRNkipzKxZswaLFy9usuyVV15Bz549UVhYiBtvvBHRaLRdAxIRERG1JqUy87vf/Q5ff/1147+3bt2KX/7yl5gyZQrmzJmD999/H4899li7hyQiIiJKJqUyU15ejsmTJzf++80338TYsWPx4osvYvbs2Zg3bx7efvvtdg9JRERElExKZaahoQFFRT98V8yKFSswbdq0xn+PHj0a+/fvb790RERERMeQUpkpKirCnj17AACxWAwbN27EKaec0jju9/thMBiS/TgRERFRu0upzPzkJz/BnDlzsHLlSsydOxdWqxWnnXZa4/iWLVvQu3fvdg9JRERElExK32X/0EMP4aKLLsLpp5+OnJwcvPzyyzAajY3jf//733H22We3e0giIiKiZNL6biav14ucnByIothkeX19PXJychoLzoEDB1BaWtrqFYM7Gq8ATEREpD2pbL/TahlOp7NZkQEAt9vdZE/NoEGDsHfv3nQegoiIiKhNOnSXyQn+hdxERESUBfh1BkRERKRpLDNERESkaSwzREREpGkdWmYEQejIX09ERETECcBERESkbSldNC9V27ZtQ2lpaUc+BBEREXVybS4zF110UZt/6YIFCwAAZWVlqSciIiIiSkGby4zT6ezIHERERERpaXOZmT9/fkfmICIiIkoLT80mIiIiTWvznpmRI0e2+VTrjRs3ph2oNY8//jjmzp2LW2+9Fc8880yHPEZbJQINUBIJCKIOYk4uBIG9UE2eBi+iCQWCIMBpM8FkMasdKS11njAicQkAYLca4LCZVE6UHr/Xi1AMEABYjUAOD1OrKh4KQgl7ASgQRAMMrkK1I6UlHglDCTYAUACdCGNusdqR0qIoMiR/AxRZhqA3QJ/jUjtS2uJ1h6BAhgAdDHklquVoc5mZMWNGB8Y4tvXr1+Ovf/0rhg0bpmoOKRxA5PuvUb/8VcTrD0G0ueAafyFsg07V9B+kVoWDQXxfFcTfFm/HN997YDaKOHt0F8w4vTcK8rTzLenhaBwHa4J4afE2bNlVA1Gnw2kjSnHZWf3RpSBH7XhtFovFUF0Xwisfbse67TUAgFMGFeLn0wagS7FL3XCdVLyhEp617yKwdQWUeBSmrv3hPvPn0LtLYLBpp2TGG6rg2/Qx/Bs/hhwNwVjUE+5JV0HMK4PJla92vDZLBDwIfPUZvGsXQQp6YXCXwn3m1TB3GwTRoqHXuqcK4e/K4Vm9AJKvFqIjH67xF8HaawQMuUUZzyMoGrgYTCAQwEknnYS//OUvePjhhzFixIg275lJ5SvEj0WRJPi3LEPtv59vNmYfMQXuyT+HaLYd12NQar7dU4P/+csayHLTP+OeJXbcd+3JyM/Xxpv1ngov7vzTZ4gl5CbLC3MteOim8SjVSKE5VO3F7HmfIxCON1lutxrw9C0TUFyojefjRBGvP4TKfz2BeM3+pgOCDqU/fwjmrgPUCZaiWH0laj/4CyL7vm42VnTpHNj6jVYhVeqkcAB1n7yEwJblzcYKzpuJnKGnQ9CJKiRLTcLfAO/6D+Bds7DZmHP8hXCOPhf6nNzjfpxUtt9pHxvxeDz4v//7P8ydOxf19fUADh9eOnjwYLq/MqmZM2fi3HPPxZQpU9r9d6ciEahH/bJXWxzzly+FFPRmOFHn5vX48LfF25sVGQDYc8iP/dUBFVKlzhOI4u1PdjQrMgBQ3RDG5l01KqRKXTwSxpJ1e5sVGQDwh+JYun4f4tGICsk6r1jNvuZFBgAUGfXLX0Pco42/LSnY0GKRAYD6pa8gVn8ow4nSI4W8LRYZAKhb+gokf0OGE6VHjgbh/eL9Fse8696HHAlmOFGaZWbLli3o168fnnjiCTz55JPweDwADl9fZu7cue2ZD2+++SY2btyIxx57rE33j0aj8Pl8TW7tRY4EW3mSFCQatPGCOlFE4gq2703+4l+/vTqDadIXDMexZVdt0vH126oQijYvCNkmEIxg/bf1ScfX76hDMBDOYCIK7Uo+fzGybzsgZ//fFfDfrEnE6yugJGIZTJO+eG3yD/ty2A8poo0PYFLQC0iJJIMJSKHMf7BPq8zMnj0b1157LXbu3Amz+YeJlj/5yU/w2WeftVu4/fv349Zbb8Vrr73W5HFa89hjj8HpdDbe2vPCfYLY+hQjwWRtt8eiYxMEwGJK/py4cgwZTJM+AYDNkjyr3WKAXsz+CeZ6vQ45raxHjkUPvT771+NEorMm3zWvM1mQ/ZMMDhOt9uSDOlETh2YAQGdufRsh6DXynqU3Htd4R0jrnWX9+vW46aabmi3v0qULKisrjzvUEV9++SWqq6tx0kknQa/XQ6/XY8WKFZg3bx70ej0kSWr2M3PnzoXX62287d/fwi7WNIlWB0ylfVoc05lzoHcUtNtj0bHl2s04Z0yXpOPjhiUfyyZFbgt+Mr5H0vGp43rAqM/+N2u704kZp3ZLOn7Bqd15VlOG5Qwan3TMPmIKxHaY15AJ5rKBQJLCYus/FtDIB0m9qwi6JFlNXQdAtGjjpAWdJQd6V8uTfPWuIoimzM/xS6vMmEymFg/f7NixAwUF7bdBnzx5MrZu3Yry8vLG26hRo3DVVVehvLwcotj8j9tkMsHhcDS5tRfR6kDB+bMg2lxNlgt6I4p/Ohd6uzbeGE4UBpMJ00/rjd5dmj/HN180CLm2Dv3qsXYjiiLGDSnB8L7Nz8i4eFIf5LssKqRKT98yJ84c2fz0zLNGlaBPaSufrqlD6CwOuM/6RbPlxpLecJx8DkSTNv62ZFMOCs+fBfzoEhgGdwlyz7gSRrtbpWSp0dvdKPrp3GZ7LsScXBROn9n6HqgsYnSXoHDG7c2Kmc5kReGFs1U5RTuts5muv/561NXV4e2334bb7caWLVsgiiJmzJiBiRMndug1YM444wzVzmY6IuGtReTQLkT3fwNDfldYeg6F3p53zMNQ1DHq6rw4WBPE+u3VcOYYMXZICXJteuQ4tPHGcERVXRDVDWGs/eoQzEY9xg8rgcNmREGuNj51HtFQ70GdP47VWw5BEIDxQ0vhtovIdbPsqyHuq4McCSD0zTpIET+svU+GwV0Mg8au0RL1eYCoH6GdG5Dw18PSYwiMBd1gdKt3bZN0KFICCV8dwnu3IF57EKaygTCX9obeoZ3TywEgHg9D8dUjsm87olV7YSruAXPXgRCchTAY2udwWSrb77TKjNfrxSWXXIINGzbA7/ejtLQUlZWVOOWUU/Dhhx/CZuu405OzocwQERFRx+rwMnPE559/js2bNzdeB0btU6dbwjJDRESkPR12nZlly5Zh0KBBjfNlJkyYgF//+tf4n//5H4wePRqDBw/GypUr009ORERElKKUyswzzzyDG264ocWG5HQ6cdNNN+Hpp59ut3BEREREx5JSmdm8eTPOOeecpONnn302vvzyy+MORURERNRWKZWZqqqqVmcp6/V61NRo4/LYREREdGJIqcx06dIFX331VdLxLVu2oKREW6fJERERkbalVGZ+8pOf4N5770Uk0vzL4sLhMO6//36cd9557RaOiIiI6FhSOjW7qqoKJ510EkRRxM0334z+/fsDAL755hs8++yzkCQJGzduRFFRy5c5VgNPzSYiItKeVLbfKV2ytqioCKtXr8avfvUrzJ07F0d6kCAImDp1Kp599tmsKjJERER04kv5+vvdu3fHv//9bzQ0NGDXrl1QFAV9+/ZFbi4vVU5ERESZl/aXCeXm5mL06NHtmYWIiIgoZWl9azYRERFRtmCZISIiIk1jmSEiIiJNY5khIiIiTWOZISIiIk1jmSEiIiJNY5khIiIiTWOZISIiIk1jmSEiIiJNY5khIiIiTWOZISIiIk1jmSEiIiJNY5khIiIiTWOZISIiIk1jmSEiIiJNY5khIiIiTWOZOQ7haBySJKkd47iFwrETYj0i0Rji8bjaMY5bJJZAPCGrHeO4RWMxxGIxtWMct0Q8cUKshyxLkOPaXw9JkpCIhtWOQUdJhINqR4Be7QBaE41LqGkIYdXmCuzY14CSPBvOGtMNbqcZDptJ7XgpOVgTwIbtldi8sxZuhxlTT+kOV44JBblWtaOlpKImgK3f1eKLbVWwWww4+5TuyHOYUJSXo3a0lByqDWDHPg9WbT4Ik0HElDHdUJJnQ1GeTe1oKamp9WJfVQBLNhwEAJw9ugvKiuwoyHOonCw1dXVeVNSF8J8vDiAaV3DmyGL06epEQb5T7WgpifvrkfBUwb/pE8jRIKx9x8DSfRAMucVqR0tJxFsHBBvgL/8EUqAB5h7DYO09Esa8UrWjdUpS0A8p5IF/6wrEa/bBUNgN9iGnQ7S6INrsGc8jKIqiZPxRM8jn88HpdMLr9cLhOP430x37GnDP86sRjiYal+l0Au762ck4uX8BLGbjcT9GJnx/yIffPv85vIGmn9T+34VDccqQYuS5tFFoDtYEcP8La1BVH2qy/OJJffCT8d1Q6M78iyodlbVBPPryF9hT4Wuy/IyTuuKqcwagWCOFpqbOh3lvb0b5rvomy0/ql4ebLxmKgjxtFIG6Oi9e/vBbLN90qMnyPl0duPvnJ2ummCX8DfCuexfede83Wa7PLUbx5ffA6C5RKVlqor56hL9Zjfol85ssF20uFF/1AEwFZSol65zi8QgSB3bg0FuPANIP20JBNKD4sruh7zoQBoPhuB8nle03DzOloLIugD+9ualJkQEAWVbwpzc3oc6njV24NQ1BvPjuV82KDAC8sGgrglFtHHLy+UP459IdzYoMALyzfBd8IW2sRywhYemGfc2KDAB8uvEAKuvU34XbVl/vrmtWZABg4446fLu3QYVE6TlQE2pWZABg1wEfVpYfQDyeaOGnso/kr2tWZAAg0VAJ79pFkGIRFVKlIRpE/ZKXmi2Wgh7Uf/Iyot7mf3PUcRRfParf+1OTIgMAihRH9XvzoPhrMp6JZSYFgXAC+6r8LY5FYhKq6rWx0QlFJWzZ1fIfm6wc3iBpgT8i4bNNB5OOrypPPpZNahvCWLZhf9LxJev2aWLjWVfvwwdrk6/H4rUHUN/Q8usnm0QiEXy0Lvl6fLSuAh5vIIOJ0hf4elXysa9WQg54MhfmOIT3bgXQ8kGE8O5yINb8Aw11HDnsh5Tkb0cKNEAOZf71wTKTAllqfVJmLK6NPQGyrKC1g4uRWPZvOAEACpBo5TmJxLTxfChQEIsnX49oXEJCzv6jwYrc+nrE4tIxX0PZQJYVRI7xfGjl4LwcjyYdUxJxKEkKQrZREq3t9VYAJfv/rk4kitz6e6siZ34bwjKTApvVCJe95Um+Op2AroXamJ9hNoroXpw867De+RlMkz6TQYcRfQuSjo8bqo35AE6bCaMHFSUdP3VEKSym4z/+3NEcDgtOHZL8+Zg4rBAuV/bP/bFaLThjRPLnY/zgAtg1MtnfNuCUpGOWXiOgM2ljbpyl+5CkY8ainoDBnME0JNqcEAwtvwYEgxmiLfNz41hmUlDosuD6C1p+Uc04vTdyrNo4OawkPwc3zBgKnU5oNjZxZBfkWLSxHvm5Nlxz7iCYDGKzsaG981CUa1EhVepyrEZceEYf5FiaF5YeJQ7075arQqrUGY1GTBxZhnxX8w1LQa4F44Z1gV6vjb+tgT3c6F7c/Gw4h82I8yf2gcWqjb8tg7sU5rJBzZYLBhPcZ1wJvQobnbRYHLANHNd8uU6E+6xfwJRbmPlMnZhgsSP39CtaHHNPuhKihWcztbv2PpupzhtGRW0Qr//nG+yt8KHQbcXFk/pgQHc3Ct3a+JQDAA3+EGoaonjjP9/g230NcNnNOP+0XhjZr0BTpwIHgiHUeON4+5Md2LyzBjkWI84Z3wMThpZoaj0kSUJFbQgLP92FdV9XwmQQceaoMkwe3Q0l+dpZDwCorPHh35/vxqfllRAEAZNGFuOccT1RXKCNM4COqK71Ydn6ffjPhoOIJ2SMH1KIGaf3QaHbpplSBgBxTzWC21fDt/FjyNEQLD2HIXfCJdC7S6DTZ/8evyNiDdUI794E3/oPIIW8MHXpj9zTLoPgLIRJhVOBO7u4pxqx2v3wrPwn4vWHYMgrhevUS2DM6wpDbvI9m6lIZfvNMpOmWk8I0bgEvU6nqY3mj9V5QojEJAgCUFqg3TeEBl8Iocjh9SjMtWhqY3O0QCgGbzAGAUB+rgVGffO9TloQCkfgDxw+U8Zus8Bq1cZhmR+Lx2Jo8B6eXJpjM8GqkT0yPybLMiRfLSDLEKw50Ju1dQ2mo0XqKyEoCmAww+TQxl7LE1ncUw1FikMQ9TC42qfEHMEyc5SOKjNERETUcXidGSIiIuo0WGaIiIhI01hmiIiISNNYZoiIiEjTWGaIiIhI01hmiIiISNNYZoiIiEjTWGaIiIhI01hmiIiISNNYZoiIiEjTWGaIiIhI01hmiIiISNNYZoiIiEjTWGaIiIhI01hmiIiISNNYZoiIiEjTWGaIiIhI01hmiIiISNNYZoiIiEjTWGaIiIhI01hmiIiISNNYZoiIiEjTWGaIiIhI01hmiIiISNNYZoiIiEjTWGaIiIhI01hmiIiISNP0agc4lsceewwLFizAN998A4vFgvHjx+OJJ55A//79VctUVR9EKJJAnScMR44JdqsBJfk5quVJVzAchycQRZ03DJvZAJfdhDynRe1YKatuCCESTaDGE4bFpIfTZkJujglWq0HtaCmp84QQikqo9YShF3VwO0xw2Iyw20xqR0uJ1x+BPxxHnScCCECe04wcix4uu7b+tqLxBOo8EdT5IkhIMgpcFrhyjMixauv5kOUEEp4aSEEPlFgEemcBdBYH9DaH2tFSVlPrgScYRygcR57LCqdFB7tTe+txopCC3sO3kBei1QnRdvimhqwvMytWrMDMmTMxevRoJBIJ3H333Tj77LOxbds22Gy2jOeprAvi2X9uRvnOmsZl3YrtmPPz0Sgrsmc8T7rqfRHMf/9rfLrxQOOywlwL7rv+FHQv1s6bQ1VdEAtXfIcPV++BrBxe5rKb8JurR6FncQ5sNrO6Aduosi6ATzcexFtLvkVCOrwiNosBt18+Ev3KnMh1WlVO2DbV9UFs2lGDFxd9hWhcAgCYjCJumjEUw/vloTBXG6U/EI5j665a/OnNjQhGEgAAvajDlVP7Y/KoMrg1UvrleBTRil2oWvAk5JDv8EJBB8eoaXCecj4Mjnx1A7aRJEnYd8iLh17agJqGMABAEIBJI0twzbQBcLu18551ooh7a1C94GlEK3Y0LjOV9kPhRbNhcBZkPE/WH2b66KOPcO2112Lw4MEYPnw4XnrpJezbtw9ffvllxrPUesKYv/jrJkUGAPZV+vHoS1/gUG0g45nSEU9IeH/l7iZFBgCqG8K457nVjW8W2S6RSGDdtkp88PkPRQYAPP4oHvi/tWgIxtULl6I9B3147aNvGosMcHjP2WMvr4c3pJ31qPVG8L//3NxYZAAgGpMw7+1y1HmjKiZLTW1DCI+/sr6xyABAQpLxyr+3Y9dBr4rJUpPw1aLyzYd/KDIAoMjwrf8AoZ0b1AuWorr6AO55YV2T9yZFAZZtPIQPVu9FPBpTMV3nI4V8qF70pyZFBgCiFTtQ896fIYX9Gc+U9WXmx7zew28kbre7xfFoNAqfz9fk1l5CkTjWbj3U4tiB6gA8AW28oBr8USxetbvFMU8giv1V7ff/rCNVN0SwYPmuFseiMQnlO2paHMs2lXUB/HPZzhbHJFnBJ+v3QZKkFsezSYMvhIWftvx8AMCiFd/B48v+ohyXZHy87nvIRzfko/xz6Q7UebN/PQAgtGsjlETL70veNYsQb6jKcKL07D3kgy/Y8nosXrMf9b5QhhN1blLIh+iB7S2ORfZ9DSmY+cKvqTIjyzJuu+02TJgwAUOGDGnxPo899hicTmfjraysrN0ePxRNIMn7GwBo5g0uFpcQiSXfOFbUBjOYJn2yAtR5I0nH91Vqo5RJkoKq+uRvxhXVQYSjiaTj2SISk1BZl3w9qupCCMeyfz1i8QQO1iR/DVTWhRCLZ3+5BIBYzf6kYwlvDaDIGUyTvoM1yT/phyIJxBPaWI8ThRxtvTwea7wjaKrMzJw5E1999RXefPPNpPeZO3cuvF5v423//uQv5lRZzQboRSHpeFGuNuY1mAx65FiST47VypwZnQ4oyUs+b6pPWW4G06TPoNe1Ot+qVxenJiad2kx6dC9Ovh7dSxywmrN+mh7MBj16dUn+GuhWbIfFlP3rAQDm0j5Jxwx5XQCdmME06etRknxSqcNmhFGvqU2Z5unMrc99O9Z4R9DMX8DNN9+MxYsXY/ny5ejatWvS+5lMJjgcjia39mK36DF5dLcWx/qWuWC3GdvtsTqS22HCpVP6tThW5LaitCDzE6vTUZqfgyumtnxWm8NmxOBeeRlOlJ5Ctw1XnN3yepiMIk4/KfnfezZx2C2YcUYf6Fro+zqdgAsm9oIzJ/snzoqiDmeO6pZ0A3nFWf3hsmtjYrm5+5CkG5bciZfB4CrMcKL0dC3MQUFuy387P53UE26NTCw/UYg2B6z9xrQ4Zu0/RpUzmrK+zCiKgptvvhkLFy7EsmXL0LNnT9Wy5DosuPTMvpgyuhvEo96xR/QrwJ0/OxnFrewlyCaH36zL8NMp/WA46g17QPdcPHTTOE2dnj2opxvXnjeoySflbsV2/O7GcehSoJ03uGK3BbdeNgL2o04nL86z4sEbxiHPro2SDADOHD3uvnYMcu0/7ElyO8z47S/GIFdD61HotuDBG8ehyP3D3laHzYjbrxiJsiLt/F3pc4tRctX9MOT/UIgFowXuydfAVDZQxWSpKch34qEbxqJvmatxmVGvw2Vn9sLE4aXQ67Wxp+xEIZpzkH/O9bANHA8I/92GCDrYBo5H/tQbIJozvy0UFEVpZRaI+n7961/j9ddfx7vvvtvk2jJOpxMWy7E3uj6fD06nE16vt9320tT5wgiFEwiE47CY9LCaRRTmaqPIHC0Wl1DvjyAYisNoEOHMMcKhsWuaAEAgFIUnEIM/FIdRr4PVrNfkdX9CoSgagjH4g3GIooAcizavXxSPx1HtiSAQikMQBORY9ChwmWEwaOu6P8DhSzH4QzHIsgK7zYgCl7XJBwCtiHtqIEcCUBJx6Kw50NvzoTNop1we4an3wheREItJyLEa4bIbYW7DdoA6hhQNQQ56IUfD0Jks0NmcEE3tN90ile131pcZQWh5jsr8+fNx7bXXHvPnO6LMEBERUcdKZfud9fvmsrxrERERkcq0t7+UiIiI6CgsM0RERKRpLDNERESkaSwzREREpGksM0RERKRpLDNERESkaSwzREREpGksM0RERKRpLDNERESkaSwzREREpGksM0RERKRpLDNERESkaSwzREREpGksM0RERKRpLDNERESkaSwzREREpGksM0RERKRpLDNERESkaSwzREREpGksM0RERKRpLDNERESkaSwzREREpGksM0RERKRpLDNERESkaSwzREREpGksM0RERKRperUDaFVFbQCyrEAnCMhzmGEyafN/5aHaICRZhiAIyLWbYDUb1I6Ulsq6IBKSDJ0gwG41wG4zqR0pLdUNQcQTMgABNpMIl8OidqS01HrCiMYlAIDZKCLPqc31aPCFEIrKABQYRR0K3Da1I6UlnpDgDUQhy4DFpIfdZlQ7UqemKDIkfwMUWYagN0Cf41I7UtoS/nooUgKCqIfe7lYthza3wCqqqgvim30NeP2jb1BRG4TLbsIFE3vjtOGlKMrTzhtddV0I+2v8eGnxNuw95IPNYsC0cd0x9ZQeKNbQetR5Q6isC2P+4q/x7fcNMBtFTB7dDRec3hslGloPXzCM6oYoXlq8DVt21UDU6XDaiFJcdlZ/dCnIUTtem8ViMRyqi+DVD7dj/fYqAMCYQUW4etpAdCt2qJwuNQeq/XhzyQ58vrkCkixjeN8CXHvuIBTnmmGzmdWO12a1njAWfroL/1n3PaIxCYN6uvHL84egR4kDRoOodrxOJxHwIPDVZ/CuXQQp6IXBXQr3mVfD3G0QRIt2XutS0Ifgrg1o+OwtSL5aiI585E68DLa+oyBaM/9aFxRFUTL+qBnk8/ngdDrh9XrhcBzf/+BAOIblGw7ghUVbm41NHlWGK6f2R6FGPrmt3lqBx15a32z58L75uPnSEZopNNt212Huc59Dlpv+GfcoceDua0ejJF8bbw57Kry480+fIZaQmywvzLXgoZvGo1QjheZAtR93zVuJQDjeZLndasDvb5mIroXaWI+KmgDueX41ajzhJstNBhFP3noaepQ4VUqWmjpvGPe/uAbfH/I3Wa7TCXhy1mnoW5arUrLOSQoHUPfJSwhsWd5srOC8mcgZejoEXfYXTDkehWf1InhWvd1szHXaT+EadyF0huPf+5fK9ptzZlLg8Ufx2n++aXFs2Zf7EY5KGU6UnoraAOa//3WLY5t31sIbiGY4UXqq6oOY/8HXzYoMAOw95MOB6oAKqVJX7wvh7U++bVZkAKC6IYzNu2pUSJW6UCSOj9d+36zIAIA/FMfS9d8jEompkCx1G7+tblZkACAal/CvZbvQ4AupkCp1ew/5mhUZAJBlBfPf/xr+kDaejxOFFPK2WGQAoG7pK5D8DRlOlB4p4IFnzYIWxzyrF0AKZn49WGZSEAzHEWzhjRoAFAWa2XhGohIq65K/GX+9uz6DadKXSCj4Zm/yF82RwxzZLhSRsGVXXdLx9duqEAxnf8H0BqLYtCN58dq0owYNwezfePqCEWxo5W9ny84aBCPa+ODyxdeVSce+2l2HiEY+gJ0o4rUHk47JYT+kiDa2IVLIC0iJJIMJSEFfZgOBZSYlRn3ru/9yLNqYPKvX66DTCUnHHTkamRwoHJ7MmIxTI5McdQJga+Vvx24xQH+Mv71sYBDFVl8DNosBRn32v+UYRF2r65FjNaKVl09WcdmTT4S3mvQQNLIeJwqd2drquKDXxjZEEFvPqcZ6ZP87SxaxmPXoW+ZqcSzHYkCeSxuTAq0mPcYMKmpxTC/q0L+bNo6ju3IMmDK6LOn4hGGlGUyTvsJcC34yvkfS8anjesBkyP65+vm5Fpx7as+k4+dN6Ik8Z+tv5tnAYjbinHE9ko5PG9cDpQX2zAU6DqcO75J07NxTe7Vadqj96V1F0Jlafg2Yug6AaNHGJHnR5oTelWQb4ipSZQIwy0wKivNsmHXZiGZvACaDiLnXjkaeXRtlJt9lwbXnDm52to9OJ+DOq06GVSN7mGwWE86f2Bu9uzSfjPn/LhwKmzn7CwAA6PV6nDKkBMP75jcbu3hSH+Q5tLPB6dvVhYkjmm9AJ53cFT1LtTFpFgAKci2YcXrvZstH9itI+kEgG+U5zfj1xcOaLe/XzYWfjO8BvchNQCbp7W4U/XQuBH3TvcZiTi4Kp8+EaNVGSdbb3Si6+K5mxUxnsqLokv9R5RRtns2UhoM1Aezc14Bvvm9A18IcjOhXAEeOEQ6rdjY6AFBRG8TeCi+2fleLfJcFowcVw24WkauBT89HO1QbREVtABu2V8GZY8IpQ4phM+k1d02QQ3UB1DaEsfarSpiNeowfVgKbRY/iPG2cAXREZV0QHn8Uq7cegiAA44eWwJlj0swZckdU1QfgDyXw+ZYKxOMSThlSgnyXWXPPRygSR70vgnVfVcIbjGHMoCKUFuTA7dDGh68TjSIlkPDVIbx3C+K1B2EqGwhzaW/oHc0/zGQzRZGR8NYicuAbxA7thrGkF8xdB0DvLIDQTscvU9l+s8wQERFR1uGp2URERNRpsMwQERGRprHMEBERkaaxzBAREZGmscwQERGRprHMEBERkaaxzBAREZGmscwQERGRprHMEBERkaaxzBAREZGmscwQERGRpmnja4WPw5GvnvL5fConISIiorY6st1uy1dInvBlxu/3AwDKyspUTkJERESp8vv9cDqdrd7nhP/WbFmWUVFRAbvd3m5fS36i8fl8KCsrw/79+/nN4lmAz0d24fORXfh8ZJeOfD4URYHf70dpaSl0utZnxZzwe2Z0Oh26du2qdgxNcDgcfHPIInw+sgufj+zC5yO7dNTzcaw9MkdwAjARERFpGssMERERaRrLDMFkMuH++++HyWRSOwqBz0e24fORXfh8ZJdseT5O+AnAREREdGLjnhkiIiLSNJYZIiIi0jSWGSIiItI0lplO6rHHHsPo0aNht9tRWFiIGTNm4Ntvv1U7Fv3X448/DkEQcNttt6kdpVM7ePAgfvaznyEvLw8WiwVDhw7Fhg0b1I7VKUmShHvvvRc9e/aExWJB79698dBDD7XpUvd0/D777DNMnz4dpaWlEAQBixYtajKuKAruu+8+lJSUwGKxYMqUKdi5c2fG8rHMdFIrVqzAzJkzsXbtWixZsgTxeBxnn302gsGg2tE6vfXr1+Ovf/0rhg0bpnaUTq2hoQETJkyAwWDAhx9+iG3btuGpp55Cbm6u2tE6pSeeeALPPfcc/vd//xfbt2/HE088gd///vf485//rHa0TiEYDGL48OF49tlnWxz//e9/j3nz5uH555/HunXrYLPZMHXqVEQikYzk49lMBACoqalBYWEhVqxYgYkTJ6odp9MKBAI46aST8Je//AUPP/wwRowYgWeeeUbtWJ3SnDlz8Pnnn2PlypVqRyEA5513HoqKivC3v/2tcdnFF18Mi8WCf/zjHyom63wEQcDChQsxY8YMAIf3ypSWluKOO+7AnXfeCQDwer0oKirCSy+9hMsvv7zDM3HPDAE4/IcHAG63W+UkndvMmTNx7rnnYsqUKWpH6fTee+89jBo1CpdeeikKCwsxcuRIvPjii2rH6rTGjx+PpUuXYseOHQCAzZs3Y9WqVZg2bZrKyWjPnj2orKxs8r7ldDoxduxYrFmzJiMZTvjvZqJjk2UZt912GyZMmIAhQ4aoHafTevPNN7Fx40asX79e7SgEYPfu3Xjuuecwe/Zs3H333Vi/fj1mzZoFo9GIa665Ru14nc6cOXPg8/kwYMAAiKIISZLwyCOP4KqrrlI7WqdXWVkJACgqKmqyvKioqHGso7HMEGbOnImvvvoKq1atUjtKp7V//37ceuutWLJkCcxms9pxCIdL/qhRo/Doo48CAEaOHImvvvoKzz//PMuMCt5++2289tpreP311zF48GCUl5fjtttuQ2lpKZ8P4mGmzu7mm2/G4sWLsXz5cn67uIq+/PJLVFdX46STToJer4der8eKFSswb9486PV6SJKkdsROp6SkBIMGDWqybODAgdi3b59KiTq3u+66C3PmzMHll1+OoUOH4uqrr8btt9+Oxx57TO1onV5xcTEAoKqqqsnyqqqqxrGOxjLTSSmKgptvvhkLFy7EsmXL0LNnT7UjdWqTJ0/G1q1bUV5e3ngbNWoUrrrqKpSXl0MURbUjdjoTJkxodrmCHTt2oHv37iol6txCoRB0uqabLFEUIcuySonoiJ49e6K4uBhLly5tXObz+bBu3TqMGzcuIxl4mKmTmjlzJl5//XW8++67sNvtjcc1nU4nLBaLyuk6H7vd3my+ks1mQ15eHucxqeT222/H+PHj8eijj+KnP/0pvvjiC7zwwgt44YUX1I7WKU2fPh2PPPIIunXrhsGDB2PTpk14+umncd1116kdrVMIBALYtWtX47/37NmD8vJyuN1udOvWDbfddhsefvhh9O3bFz179sS9996L0tLSxjOeOpxCnRKAFm/z589XOxr91+mnn67ceuutasfo1N5//31lyJAhislkUgYMGKC88MILakfqtHw+n3Lrrbcq3bp1U8xms9KrVy/lt7/9rRKNRtWO1iksX768xW3GNddcoyiKosiyrNx7771KUVGRYjKZlMmTJyvffvttxvLxOjNERESkaZwzQ0RERJrGMkNERESaxjJDREREmsYyQ0RERJrGMkNERESaxjJDREREmsYyQ0RERJrGMkNERESaxjJD1IkJgoBFixapHeO4PfDAAxgxYkSr9znjjDNw2223ten3ffrppxAEAR6P57izEVHHY5khOoFVVlbilltuQa9evWAymVBWVobp06c3+UK4bHakVLR2+/TTT9v0uxYsWICHHnqoYwMTkSr4RZNEJ6i9e/diwoQJcLlc+MMf/oChQ4ciHo/jP//5D2bOnIlvvvlG7YjHNH78eBw6dKjx37feeit8Ph/mz5/fuMztdrep0Ljd7o6ISERZgHtmiE5Qv/71ryEIAr744gtcfPHF6NevHwYPHozZs2dj7dq1ze7f0qGV8vJyCIKAvXv3AgBeeukluFwuLF68GP3794fVasUll1yCUCiEl19+GT169EBubi5mzZoFSZIaf0+PHj3w0EMP4YorroDNZkOXLl3w7LPPHnMdjEYjiouLG28WiwUmk6nJMqPR2Hj/V199FT169IDT6cTll18Ov9/fOPbjw0zRaBS/+c1vUFZWBpPJhD59+uBvf/tbizlCoRCmTZuGCRMmwOPxYO/evRAEAQsWLMCkSZNgtVoxfPhwrFmzpsnPrVq1CqeddhosFgvKysowa9YsBIPBxvG//OUv6Nu3L8xmM4qKinDJJZc0jv3rX//C0KFDYbFYkJeXhylTpjT5WSL6AcsM0Qmovr4eH330EWbOnAmbzdZs3OVypf27Q6EQ5s2bhzfffBMfffQRPv30U1x44YX497//jX//+9949dVX8de//hX/+te/mvzcH/7wBwwfPhybNm3CnDlzcOutt2LJkiVp5/ix7777DosWLcLixYuxePFirFixAo8//njS+//85z/HG2+8gXnz5mH79u3461//ipycnGb383g8OOussyDLMpYsWdLk/91vf/tb3HnnnSgvL0e/fv1wxRVXIJFINOY555xzcPHFF2PLli146623sGrVKtx8880AgA0bNmDWrFn43e9+h2+//RYfffQRJk6cCAA4dOgQrrjiClx33XXYvn07Pv30U1x00UXg9wITJZGx7+cmooxZt26dAkBZsGBBq/cDoCxcuFBRFEVZvny5AkBpaGhoHN+0aZMCQNmzZ4+iKIoyf/58BYCya9euxvvcdNNNitVqVfx+f+OyqVOnKjfddFPjv7t3766cc845TR77sssuU6ZNm5bSel1zzTXKBRdc0Gz5/fffr1itVsXn8zUuu+uuu5SxY8c2/vv0009Xbr31VkVRFOXbb79VAChLlixp8XGO/L/Yvn27MmzYMOXiiy9WotFo4/iePXsUAMr//d//NS77+uuvG39GURTll7/8pXLjjTc2+b0rV65UdDqdEg6HlXfeeUdxOBxNMh/x5ZdfKgCUvXv3Hvt/ChEp3DNDdAJSOvATvNVqRe/evRv/XVRUhB49ejTZq1FUVITq6uomPzdu3Lhm/96+fXu75erRowfsdnvjv0tKSpplOKK8vByiKOL0009v9XeeddZZ6NOnD956660mh7OOGDZsWJPHA9D4mJs3b8ZLL72EnJycxtvUqVMhyzL27NmDs846C927d0evXr1w9dVX47XXXkMoFAIADB8+HJMnT8bQoUNx6aWX4sUXX0RDQ0Nq/0OIOhGWGaITUN++fSEIQkqTfHW6w28HRxeheDze7H4Gg6HJvwVBaHGZLMupRD5uqWSwWCxt+p3nnnsuPvvsM2zbtu2YjykIAgA0PmYgEMBNN92E8vLyxtvmzZuxc+dO9O7dG3a7HRs3bsQbb7yBkpIS3HfffRg+fDg8Hg9EUcSSJUvw4YcfYtCgQfjzn/+M/v37Y8+ePW3KTdTZsMwQnYDcbjemTp2KZ599tsVJoy1dP6WgoAAAmpw9VF5e3m6ZfjzpeO3atRg4cGC7/f5UDB06FLIsY8WKFa3e7/HHH8c111yDyZMnJy00yZx00knYtm0b+vTp0+x2ZC+PXq/HlClT8Pvf/x5btmzB3r17sWzZMgCHy9GECRPw4IMPYtOmTTAajVi4cGF6K0x0gmOZITpBPfvss5AkCWPGjME777yDnTt3Yvv27Zg3b16zQz4A0KdPH5SVleGBBx7Azp078cEHH+Cpp55qtzyff/45fv/732PHjh149tln8c9//hO33npru/3+VPTo0QPXXHMNrrvuOixatAh79uzBp59+irfffrvZfZ988klcddVVOPPMM1Pa0/Wb3/wGq1evxs0334zy8nLs3LkT7777buME4MWLF2PevHkoLy/H999/j1deeQWyLKN///5Yt24dHn30UWzYsAH79u3DggULUFNTo1r5I8p2vM4M0QmqV69e2LhxIx555BHccccdOHToEAoKCnDyySfjueeea3Z/g8GAN954A7/61a8wbNgwjB49Gg8//DAuvfTSdslzxx13YMOGDXjwwQfhcDjw9NNPY+rUqe3yu9Px3HPP4e6778avf/1r1NXVoVu3brj77rtbvO8f//hHSJKEM888E59++mmL82d+bNiwYVixYgV++9vf4rTTToOiKOjduzcuu+wyAIfPKFuwYAEeeOABRCIR9O3bF2+88QYGDx6M7du347PPPsMzzzwDn8+H7t2746mnnsK0adPa9f8B0YlCUDpypiAREQ7vCbntttva/HUCRESp4GEmIiIi0jSWGSJS1Wuvvdbk9OWjb4MHD1Y7HhFpAA8zEZGq/H4/qqqqWhwzGAzo3r17hhMRkdawzBAREZGm8TATERERaRrLDBEREWkaywwRERFpGssMERERaRrLDBEREWkaywwRERFpGssMERERaRrLDBEREWna/wca8wG1u60PTQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(x = X1['Clump_Thickness'], y = X1['Cell_Size_Uniformity'], hue = y1, palette = \"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen Algorithm\n",
    "\n",
    "For this assignment, we decided to use KNN. [[more about K-Nearest Neighbors]](#knnk-nearest-neighbors)\n",
    "\n",
    "In our initial approach, we will implement the algorithm without modifications and analyze the outcomes. Sencondly, we aim to enhance these results by implementing a modified algorithm for comparison. We will adjust the number of k-neighbors, explore different distance methods, and experiment with various features to achieve this improvement.\n",
    "\n",
    "[[go back to the top]](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "[[go back to the topic]](#chosen-algorithm)\n",
    "\n",
    "The metrics.py file contains various performance evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "\n",
    "EPS = 1e-15\n",
    "\n",
    "\n",
    "def unhot(function):\n",
    "    \"\"\"Convert one-hot representation into one column.\"\"\"\n",
    "\n",
    "    def wrapper(actual, predicted):\n",
    "        if len(actual.shape) > 1 and actual.shape[1] > 1:\n",
    "            actual = actual.argmax(axis=1)\n",
    "        if len(predicted.shape) > 1 and predicted.shape[1] > 1:\n",
    "            predicted = predicted.argmax(axis=1)\n",
    "        return function(actual, predicted)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def absolute_error(actual, predicted):\n",
    "    return np.abs(actual - predicted)\n",
    "\n",
    "\n",
    "@unhot\n",
    "def classification_error(actual, predicted):\n",
    "    return (actual != predicted).sum() / float(actual.shape[0])\n",
    "\n",
    "\n",
    "@unhot\n",
    "def accuracy(actual, predicted):\n",
    "    return 1.0 - classification_error(actual, predicted)\n",
    "\n",
    "\n",
    "def mean_absolute_error(actual, predicted):\n",
    "    return np.mean(absolute_error(actual, predicted))\n",
    "\n",
    "\n",
    "def squared_error(actual, predicted):\n",
    "    return (actual - predicted) ** 2\n",
    "\n",
    "\n",
    "def squared_log_error(actual, predicted):\n",
    "    return (np.log(np.array(actual) + 1) - np.log(np.array(predicted) + 1)) ** 2\n",
    "\n",
    "\n",
    "def mean_squared_log_error(actual, predicted):\n",
    "    return np.mean(squared_log_error(actual, predicted))\n",
    "\n",
    "\n",
    "def mean_squared_error(actual, predicted):\n",
    "    return np.mean(squared_error(actual, predicted))\n",
    "\n",
    "\n",
    "def root_mean_squared_error(actual, predicted):\n",
    "    return np.sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "\n",
    "def root_mean_squared_log_error(actual, predicted):\n",
    "    return np.sqrt(mean_squared_log_error(actual, predicted))\n",
    "\n",
    "\n",
    "def logloss(actual, predicted):\n",
    "    predicted = np.clip(predicted, EPS, 1 - EPS)\n",
    "    loss = -np.sum(actual * np.log(predicted))\n",
    "    return loss / float(actual.shape[0])\n",
    "\n",
    "\n",
    "def hinge(actual, predicted):\n",
    "    return np.mean(np.max(1.0 - actual * predicted, 0.0))\n",
    "\n",
    "\n",
    "def binary_crossentropy(actual, predicted):\n",
    "    predicted = np.clip(predicted, EPS, 1 - EPS)\n",
    "    return np.mean(-np.sum(actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted)))\n",
    "\n",
    "\n",
    "# aliases\n",
    "mse = mean_squared_error\n",
    "rmse = root_mean_squared_error\n",
    "mae = mean_absolute_error\n",
    "\n",
    "\n",
    "def get_metric(name):\n",
    "    \"\"\"Return metric function by name\"\"\"\n",
    "    try:\n",
    "        return globals()[name]\n",
    "    except Exception:\n",
    "        raise ValueError(\"Invalid metric function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distances\n",
    "[[go back to the topic]](#chosen-algorithm)\n",
    "\n",
    "The inicial github file provides these functions: \n",
    "\n",
    "- the Euclidean distance \n",
    "\n",
    "- the L2 distance matrix for a set of points in a dataset.\n",
    "  \n",
    "But, as said above, we'll explore different distance methods, like:\n",
    "- Euclidean distance (features)\n",
    "- Cosine Similarity\n",
    "- Manhattan Distance\n",
    "- Jaccard Distance\n",
    "- Mahalanobis distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidian Distance\n",
    "[[go back to the topic]](#distances)\n",
    "\n",
    "This function is designed to calculate the distance between two individual points p1 and p2.\n",
    "Calculates the straight-line distance between two points in Euclidean space, denoted as p1 and p2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "def euclidean_distance(a, b):\n",
    "    if isinstance(a, list) and isinstance(b, list):\n",
    "        a = np.array(a)\n",
    "        b = np.array(b)\n",
    "\n",
    "    return math.sqrt(sum((a - b) ** 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Distance\n",
    "[[go back to the topic]](#distances)\n",
    "\n",
    "\n",
    "The function l2_distance(X) calculates the pairwise L2 or Euclidean distances between rows of a matrix X. This matrix X is expected to be a two-dimensional NumPy array where each row represents a point in a multidimensional space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_distance(X):\n",
    "    sum_X = np.sum(X * X, axis=1)\n",
    "    return (-2 * np.dot(X, X.T) + sum_X).T + sum_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidian Distance (features)\n",
    "[[go back to the topic]](#distances)\n",
    "\n",
    "This function is designed to calculate the Euclidean distances between a single point p1 and multiple points p2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_features(p1, p2):\n",
    "    temp = p1 - p2[:, np.newaxis]\n",
    "    euclid_dist = np.sqrt(np.sum(temp ** 2, axis=-1))\n",
    "    return euclid_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "[[go back to the topic]](#distances)\n",
    "\n",
    "The function cosine_similarity(row1, row2) calculates the cosine similarity between two vectors represented by row1 and row2. Cosine similarity measures the cosine of the angle between two vectors in a multi-dimensional space, providing an indication of how similar the vectors are in terms of orientation, regardless of their magnitude.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(row1, row2):\n",
    "    norm1 = np.linalg.norm(row1)\n",
    "    norm2 = np.linalg.norm(row2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0  # avoid zero division\n",
    "    simi = np.dot(row1, row2) / (norm1 * norm2)\n",
    "    return simi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manhattan Distance\n",
    "[[go back to the topic]](#distances)\n",
    "\n",
    "The function calculates the distance between two vectors a and b.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance(a,b):\n",
    "    return sum(abs(val1-val2) for val1,val2 in zip(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Distance\n",
    "[[go back to the topic]](#distances)\n",
    "\n",
    "The function computes the Jaccard distance between two lists, which represent sets of elements. The Jaccard distance is a measure of dissimilarity between two sets based on the size of their intersection divided by the size of their union.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_distance(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1)+len(list2))-intersection\n",
    "    return float(intersection)/union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mahalanobis distance\n",
    "[[go back to the topic]](#distances)\n",
    "\n",
    "This function computes the Mahalanobis distance that measures the distance between a point and a distribution, considering both the variance and covariance of the distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis_distance(x, x_train, epsilon=1e-8):\n",
    "    \"\"\"Calcula a dist√¢ncia de Mahalanobis entre x e os dados x_train.\"\"\"\n",
    "    # Se `x_train` n√£o tiver linhas suficientes, retorne uma dist√¢ncia alta\n",
    "    if x_train.ndim != 2 or x_train.shape[0] < 2:\n",
    "        return float('inf')\n",
    "    \n",
    "    covariance_matrix = np.cov(x_train, rowvar=False)\n",
    "    inv_covmat = np.linalg.inv(covariance_matrix + epsilon * np.eye(covariance_matrix.shape[0]))\n",
    "    diff = x - np.mean(x_train, axis=0)\n",
    "    dist = np.sqrt(np.dot(np.dot(diff, inv_covmat), diff.T))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base\n",
    "[[go back to the top]](#table-of-contents)\n",
    "\n",
    "Here, we present the implementation of the original algorithm without any modifications.\n",
    "\n",
    "This BaseEstimator python script is used for creating estimators. Includes methods for configuring input data, helping models, and making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import numpy as np\n",
    "\n",
    "class BaseEstimator:\n",
    "    y_required = True\n",
    "    fit_required = True\n",
    "\n",
    "    def _setup_input(self, X, y=None):\n",
    "        \"\"\"Ensure inputs to an estimator are in the expected format.\n",
    "\n",
    "        Ensures X and y are stored as numpy ndarrays by converting from an\n",
    "        array-like object if necessary. Enables estimators to define whether\n",
    "        they require a set of y target values or not with y_required, e.g.\n",
    "        kmeans clustering requires no target labels and is fit against only X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like\n",
    "            Feature dataset.\n",
    "        y : array-like\n",
    "            Target values. By default is required, but if y_required = false\n",
    "            then may be omitted.\n",
    "        \"\"\"\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.array(X)\n",
    "\n",
    "        if X.size == 0:\n",
    "            raise ValueError(\"Got an empty matrix.\")\n",
    "\n",
    "        if X.ndim == 1:\n",
    "            self.n_samples, self.n_features = 1, X.shape\n",
    "        else:\n",
    "            self.n_samples, self.n_features = X.shape[0], np.prod(X.shape[1:])\n",
    "\n",
    "        self.X = X\n",
    "\n",
    "        if self.y_required:\n",
    "            if y is None:\n",
    "                raise ValueError(\"Missed required argument y\")\n",
    "\n",
    "            if not isinstance(y, np.ndarray):\n",
    "                y = np.array(y)\n",
    "\n",
    "            if y.size == 0:\n",
    "                raise ValueError(\"The targets array must be no-empty.\")\n",
    "\n",
    "        self.y = y\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._setup_input(X, y)\n",
    "\n",
    "    def predict(self, X=None):\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.array(X)\n",
    "\n",
    "        if self.X is not None or not self.fit_required:\n",
    "            return self._predict(X)\n",
    "        else:\n",
    "            raise ValueError(\"You must call `fit` before `predict`\")\n",
    "\n",
    "    def _predict(self, X=None):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original KNN\n",
    "[[go back to the topic]](#base)\n",
    "\n",
    "This Python code defines a base framework and specific implementations for the k-nearest neighbors (KNN) algorithm, with a classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "\n",
    "class KNNBase(BaseEstimator):\n",
    "    def __init__(self, k=7, distance_func=euclidean_distance): # default\n",
    "        \"\"\"Base class for Nearest neighbors classifier and regressor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        k : int, default 5\n",
    "            The number of neighbors to take into account. If 0, all the\n",
    "            training examples are used.\n",
    "        distance_func : function, default euclidean distance\n",
    "            A distance function taking two arguments. Any function from\n",
    "            scipy.spatial.distance will do.\n",
    "        \"\"\"\n",
    "\n",
    "        self.k = None if k == 0 else k  # l[:None] returns the whole list\n",
    "        self.distance_func = distance_func\n",
    "\n",
    "    def aggregate(self, neighbors_targets):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _predict(self, X=None):\n",
    "        predictions = [self._predict_x(x) for x in X]\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict_x(self, x):\n",
    "        \"\"\"Predict the label of a single instance x.\"\"\"\n",
    "\n",
    "        # compute distances between x and all examples in the training set.\n",
    "        distances = (self.distance_func(x, example) for example in self.X )\n",
    "\n",
    "        # Sort all examples by their distance to x and keep their target value.\n",
    "        neighbors = sorted(((dist, target) for (dist, target) in zip(distances, self.y)), key=lambda x: x[0])\n",
    "\n",
    "        # Get targets of the k-nn and aggregate them (most common one or\n",
    "        # average).\n",
    "        neighbors_targets = [target for (_, target) in neighbors[: self.k]]\n",
    "\n",
    "        return self.aggregate(neighbors_targets)\n",
    "\n",
    "\n",
    "class KNNClassifier(KNNBase):\n",
    "    \"\"\"Nearest neighbors classifier.\n",
    "\n",
    "    Note: if there is a tie for the most common label among the neighbors, then\n",
    "    the predicted label is arbitrary.\"\"\"\n",
    "\n",
    "    def aggregate(self, neighbors_targets):\n",
    "        \"\"\"Return the most common target label.\"\"\"\n",
    "\n",
    "        most_common_label = Counter(neighbors_targets).most_common(1)[0][0]\n",
    "        return most_common_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "This Python script defines and executes a classification task using a k-nearest neighbors (KNN) algorithm from a synthetic dataset.\n",
    "\n",
    "[[bo back to the topic]](#base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification accuracy 0.96\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "def classification():\n",
    "    X, y = make_classification(\n",
    "        n_samples=500,\n",
    "        n_features=5,\n",
    "        n_informative=5,\n",
    "        n_redundant=0,\n",
    "        n_repeated=0,\n",
    "        n_classes=2,\n",
    "        random_state=1111,\n",
    "        class_sep=1.5,\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1111)\n",
    "\n",
    "    clf = KNNClassifier(k=7, distance_func=euclidean_distance)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    print(\"classification accuracy\", accuracy(y_test, predictions))\n",
    "    #print(classification_report(y_test, predictions))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Algorithm\n",
    "[[go back to the top]](#table-of-contents)\n",
    "\n",
    "In this chapter we will approach bagging [[more about bagging]](#bagging)\n",
    "\n",
    "In machine learning, blending different methods often boosts resilience. By merging KNN Bagging and KNN Features, we harness ensemble learning and feature customization for more robust models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging \n",
    "[[go back to the topic]](#improved-algorithm)\n",
    "\n",
    "**Bagging**, short for Bootstrap Aggregating is an ensemble technique that improves model stability and accuracy by training multiple models on different subsets of data and combining their predictions. This method reduces variance, enhances performance, and minimizes overfitting.\n",
    "\n",
    "\n",
    "The `KNNBagging` class integrates the principles of bagging with the k-Nearest Neighbors (k-NN) algorithm. By allowing the selection of various distance metrics, it enhances the adaptability and precision of the k-NN method:\n",
    "\n",
    "After calculating distances using various metrics, the k nearest neighbors are identified for each test instance and the most common label among them are selected. This technique aggregates outcomes from different sub-models, effectively reducing prediction variance and improving accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classe que eu acho que esta funcional \n",
    "class KNN_Bagging:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def _get_distances(self, x, distance_func):\n",
    "        return [distance_func(x, x_train) for x_train in self.X_train]\n",
    "\n",
    "    def _get_labels(self, distances):\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        return [self.y_train[idx] for idx in k_indices]\n",
    "\n",
    "    def _most_frequent_label(self, labels):\n",
    "        unique_classes, counts = np.unique(labels, return_counts=True)\n",
    "        return unique_classes[np.argmax(counts)]\n",
    "\n",
    "    def predict(self, X, metric_index):\n",
    "        distance_functions = [euclidean_distance, cosine_similarity, manhattan_distance, jaccard_distance,mahalanobis_distance]\n",
    "        predictions = []\n",
    "\n",
    "        for x in X:\n",
    "            distances = self._get_distances(x, distance_functions[metric_index])\n",
    "            labels = self._get_labels(distances)\n",
    "            predictions.append(self._most_frequent_label(labels))\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    def predict_probabilities(self, X):\n",
    "        y_proba = []\n",
    "        for x in X:\n",
    "            distances = np.sqrt(np.sum((self.X_train - x) ** 2, axis=1))\n",
    "            sorted_indices = np.argsort(distances)[:self.k]\n",
    "            labels = self.y_train[sorted_indices]\n",
    "            unique_classes, counts = np.unique(labels, return_counts=True)\n",
    "            class_frequencies = counts / self.k\n",
    "            y_proba.append(class_frequencies)\n",
    "\n",
    "        return np.array(y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN-Features\n",
    "[[go back to the topic]](#improved-algorithm)\n",
    "\n",
    "The `KNN_Features` class optimizes the k-Nearest Neighbors algorithm by prioritizing influential features in determining data point proximity. It calculates distances with a chosen metric, identifies the k nearest neighbors, and selects the most prevalent label among them, boosting prediction accuracy. Furthermore, it estimates class probabilities based on nearest neighbor label frequencies, offering a quantifiable prediction confidence measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classe do ano passado \n",
    "class KNN_Features:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict(x) for x in X]\n",
    "        return predictions\n",
    "    \n",
    "    def _predict(self, x):\n",
    "        predictions = list()\n",
    "        distances = [euclidean_distance_features(x, x_train) for x_train in self.X_train]\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = [self.y_train.reset_index(drop=True)[i] for i in k_indices]\n",
    "        k_nearest_labels = list(self.y_train.reset_index(drop=True).iloc[k_indices.ravel()])\n",
    "        unique_classes,counts=np.unique(k_nearest_labels, return_counts=True)\n",
    "        most_frequent_label=unique_classes[np.argmax(counts)]\n",
    "        most_common_str = str(most_frequent_label)\n",
    "        predictions.append(most_common_str)\n",
    "        return predictions\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        y_proba = []\n",
    "        for sample in X:\n",
    "            distances = np.sqrt(np.sum(((self.X_train - sample)) ** 2, axis=1))\n",
    "            sorted_indices = np.argsort(distances)\n",
    "            k_indices = sorted_indices[:self.k]\n",
    "            k_nearest_classes = self.y_train[k_indices]\n",
    "            unique_classes, counts = np.unique(k_nearest_classes, return_counts=True)\n",
    "            class_frequencies = counts / self.k  # relative frequency of the classes\n",
    "            y_proba.append(class_frequencies)\n",
    "        return np.array(y_proba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Training is a critical process in machine learning where a model learns from a dataset to identify patterns and make informed decisions. This process is essential as it equips the model to accurately predict outcomes on new, unseen data.\n",
    "\n",
    "We will implement three distinct training approaches to optimize our model's performance:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base training\n",
    "[[go back to the topic]](#chosen-algorithm)\n",
    "\n",
    "Tests on the Original k-NN Implementation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\naccuracy, y_test, predictions, X_test, clf = train_base(df1, k=5, test_size=0.3)\\n'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_base(df, k, test_size):\n",
    "    target_column='Class'\n",
    "    y = df[target_column].to_numpy()\n",
    "    X = df.drop(columns=[target_column])\n",
    "    numeric_cols = [c for c in X.columns if is_numeric_dtype(X[c])]\n",
    "    X = X[numeric_cols].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=1234)\n",
    "    clf = KNNClassifier(k=k)\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return accuracy, y_test, predictions, X_test, clf,'Eucledian'\n",
    "'''\n",
    "accuracy, y_test, predictions, X_test, clf = train_base(df1, k=5, test_size=0.3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Training\n",
    "[[go back to the topic]](#improved-algorithm)\n",
    "\n",
    "The function creates a bagging ensemble of k-nearest neighbors classifiers on a given dataset, trained on various distance metrics including Euclidean, Cosine, Manhattan, Jaccard, and Mahalanobis. It assesses the accuracy of the ensemble using the specified distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bagging(dataset, k, test_split, dist):\n",
    "    \n",
    "    target_column = 'Class'\n",
    "    y = dataset[target_column].to_numpy()\n",
    "    X = dataset.drop(columns=[target_column])\n",
    "    numeric_cols = [c for c in X.columns if is_numeric_dtype(X[c])]\n",
    "    X_numeric = X[numeric_cols].values\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_numeric, y, test_size=test_split, random_state=1234)\n",
    "\n",
    "    clf = KNN_Bagging(k)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    predictions = clf.predict(X_test, dist)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    #print (accuracy)\n",
    "\n",
    "    if dist == 0:\n",
    "        dist_name = 'Euclidean'\n",
    "    elif dist == 1:\n",
    "        dist_name = 'Cosine'\n",
    "    elif dist == 2:\n",
    "        dist_name = 'Manhattan'\n",
    "    elif dist == 3: \n",
    "        dist_name = 'Jaccard'\n",
    "    else: \n",
    "        dist_name = 'Mahalanobis'\n",
    "\n",
    "    return accuracy, y_test, predictions, X_test, clf, dist_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9658536585365853,\n",
       " array([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0]),\n",
       " [1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " array([[201.,   9.,   7., ...,   7.,   8.,   3.],\n",
       "        [559.,   2.,   1., ...,   2.,   1.,   1.],\n",
       "        [673.,   2.,   1., ...,   3.,   1.,   1.],\n",
       "        ...,\n",
       "        [ 94.,   1.,   1., ...,   2.,   1.,   1.],\n",
       "        [339.,   1.,   1., ...,   2.,   1.,   1.],\n",
       "        [527.,   4.,   1., ...,   1.,   1.,   1.]]),\n",
       " <__main__.KNN_Bagging at 0x30753af10>,\n",
       " 'Manhattan')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_bagging_sameK(dataset, k, test_split):\n",
    "    best_accuracy = 0\n",
    "    best_result = None\n",
    "\n",
    "    # Iterate over 5 different distance metrics (0 to 4)\n",
    "    for dist in range(5): \n",
    "        accuracy, y_test, predictions, X_test, clf, dist_name = train_bagging(dataset, k, test_split, dist)\n",
    "\n",
    "        # Check if the current accuracy is the highest found so far\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_result = {\n",
    "                'accuracy': accuracy,\n",
    "                'distance_metric': dist_name,\n",
    "                'y_test': y_test,\n",
    "                'predictions': predictions,\n",
    "                'X_test': X_test,\n",
    "                'classifier': clf\n",
    "            }\n",
    "\n",
    "    # Return the details of the best result\n",
    "    if best_result:\n",
    "        return (best_result['accuracy'], best_result['y_test'], best_result['predictions'], \n",
    "                best_result['X_test'], best_result['classifier'], best_result['distance_metric'])\n",
    "    else:\n",
    "        return None  # Or appropriate defaults/exceptions if no results are found\n",
    "    \n",
    "train_bagging_sameK(df1,3,0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-NN Feature Ensemble\n",
    "This function constructs an ensemble of k-Nearest Neighbors (k-NN) classifiers, each trained on a **randomly selected subset of features from the data**. This approach diversifies the training process by using different feature combinations for each classifier, enhancing accuracy of the ensemble. The classifiers are then compiled, and the most effective one, determined by frequency of selection, is chosen to represent the ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "def ensemble_KNN_Bagging(X, y, k, n_classifiers, test_size):\n",
    "    \"\"\"Cria um conjunto de classificadores KNN usando bagging.\"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    feature_subsets = [np.random.choice(range(n_features), int(test_size * n_features), replace=False) for _ in range(n_classifiers)]\n",
    "    classifiers = []\n",
    "\n",
    "    for features in feature_subsets:\n",
    "        X_subset = X[:, features]\n",
    "        X_resampled, y_resampled = resample(X_subset, y, n_samples=int(test_size * n_samples), replace=True, random_state=42)\n",
    "        cls = KNN_Features(k)\n",
    "        cls.fit(X_resampled, y_resampled)\n",
    "        classifiers.append(cls)\n",
    "    ensemble= Counter(classifiers).most_common(1)[0][0]\n",
    "    return ensemble,cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Training \n",
    "\n",
    "Train an ensemble of k-NN models using `ensemble_KNN_Bagging` on the training set, where each model employs a unique feature subset to enhance generalization and reduce overfitting. Then, use this ensemble to predict outcomes on the test set and assess its performance by calculating accuracy in capturing dataset patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/qx/kgqzhwb50b7flqgr05m062t40000gn/T/ipykernel_6708/4146469709.py\", line 19, in <module>\n",
      "    accuracy, y_test, pred_distances, X_test, clf = train_features(df1, 5, test_size= 0.3)\n",
      "  File \"/var/folders/qx/kgqzhwb50b7flqgr05m062t40000gn/T/ipykernel_6708/4146469709.py\", line 14, in train_features\n",
      "    predictions = ensemble.predict( X_test)\n",
      "  File \"/var/folders/qx/kgqzhwb50b7flqgr05m062t40000gn/T/ipykernel_6708/3282087807.py\", line 11, in predict\n",
      "    predictions = [self._predict(x) for x in X]\n",
      "  File \"/var/folders/qx/kgqzhwb50b7flqgr05m062t40000gn/T/ipykernel_6708/3282087807.py\", line 11, in <listcomp>\n",
      "    predictions = [self._predict(x) for x in X]\n",
      "  File \"/var/folders/qx/kgqzhwb50b7flqgr05m062t40000gn/T/ipykernel_6708/3282087807.py\", line 18, in _predict\n",
      "    k_nearest_labels = [self.y_train.reset_index(drop=True)[i] for i in k_indices]\n",
      "  File \"/var/folders/qx/kgqzhwb50b7flqgr05m062t40000gn/T/ipykernel_6708/3282087807.py\", line 18, in <listcomp>\n",
      "    k_nearest_labels = [self.y_train.reset_index(drop=True)[i] for i in k_indices]\n",
      "AttributeError: 'numpy.ndarray' object has no attribute 'reset_index'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 799, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 845, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/stack_data/core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/pygments/style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/pygments/style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "def train_features(df, k, test_size):\n",
    "    \n",
    "    target_column='Class'\n",
    "    y = df[target_column].to_numpy()\n",
    "    X = df.drop(columns=[target_column])\n",
    "\n",
    "    numeric_cols = [c for c in X.columns if is_numeric_dtype(X[c])]\n",
    "    X = X[numeric_cols].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=1234)\n",
    "    \n",
    "    ensemble,clf = ensemble_KNN_Bagging(X_train, y_train, k, 5, test_size)\n",
    "\n",
    "    predictions = ensemble.predict( X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    #retornar d que √© a melhor distancia tb\n",
    "    return accuracy, y_test, predictions, X_test, clf, X, y, y_train,d\n",
    "#teste\n",
    "accuracy, y_test, pred_distances, X_test, clf = train_features(df1, 5, test_size= 0.3)\n",
    "print (y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters test\n",
    "\n",
    "We are now going to test different parameters all combined. This involves evaluating the three types of k-nearest neighbors (k-NN) methods‚Äîbase, bagging, and feature enhanced‚Äîacross various combinations of k values, test splits, and distance functions achieving the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_knn_parameters(dataset, dataset_name, knn_type, kmax=10, test_splits=[0.2, 0.3, 0.4]):\n",
    "    best_accuracy = 0\n",
    "    best_k = None\n",
    "    best_test_split = None\n",
    "    best_distance = None\n",
    "    results = []\n",
    "\n",
    "    for test_split in test_splits:\n",
    "        for k in range(3, kmax + 1):\n",
    "            if knn_type == 'base':\n",
    "                accuracy, y_test, predictions, _, _, distance = train_base(dataset, k, test_split)\n",
    "                precision = precision_score(y_test, predictions, average='weighted', zero_division=1)\n",
    "                recall=recall_score(y_test, predictions, average='weighted')\n",
    "                cm1 = confusion_matrix(y_test, predictions)\n",
    "                sensitivity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "                results.append((dataset_name, knn_type, test_split, k, distance, accuracy,precision,recall,sensitivity))\n",
    "            elif knn_type == 'bagging':\n",
    "                for d in range(5):\n",
    "                    accuracy, y_test, predictions, _, _, distance = train_bagging(dataset, k, test_split, d)\n",
    "                    precision = precision_score(y_test, predictions, average='weighted', zero_division=1)\n",
    "                    recall=recall_score(y_test, predictions, average='weighted')\n",
    "                    cm1 = confusion_matrix(y_test, predictions)\n",
    "                    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "                    results.append((dataset_name, knn_type, test_split, k, distance, accuracy,precision,recall,sensitivity))\n",
    "                    \n",
    "            elif knn_type == 'features':\n",
    "                accuracy, y_test, predictions, _, _, distance = train_features(dataset, k, test_split)\n",
    "                precision = precision_score(y_test, predictions, average='weighted', zero_division=1)\n",
    "                recall=recall_score(y_test, predictions, average='weighted')\n",
    "                cm1 = confusion_matrix(y_test, predictions)\n",
    "                sensitivity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "                results.append((dataset_name, knn_type, test_split, k, distance, accuracy,precision,recall,sensitivity))\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe that working with CSV files simplifies data handling and analysis. After running this function, it generates a CSV file that collects dataset identifiers, k-NN types, test splits, k values, distances, and their corresponding accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_to_csv(all_results):\n",
    "    filename = 'test_parameters.csv'\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['dataset_name', 'knn_type', 'test_split', 'k', 'distance', 'accuracy','precision','recall','sensitivity']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for result in all_results:\n",
    "            writer.writerow({\n",
    "                'dataset_name': result[0],\n",
    "                'knn_type': result[1],\n",
    "                'test_split': result[2],\n",
    "                'k': result[3],\n",
    "                'distance': result[4],\n",
    "                'accuracy': result[5],\n",
    "                'precision':result[6],\n",
    "                'recall':result[7],\n",
    "                'sensitivity':result[8]\n",
    "            })\n",
    "\n",
    "datasets = [df1]  # falta os outros datasets\n",
    "knn_types = ['base', 'bagging']  # falta features\n",
    "all_results = []\n",
    "\n",
    "for i, dataset in enumerate(datasets, start=1):\n",
    "    for knn_type in knn_types:\n",
    "        results = test_knn_parameters(dataset, i, knn_type)\n",
    "        all_results.extend(results)\n",
    "        #print(f\"Results Extended: {results}\")  # Debug print\n",
    "\n",
    "write_results_to_csv(all_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a glimpse of the dataset generated:\n",
      " \n",
      "   dataset_name knn_type  test_split  k   distance  accuracy  precision   \n",
      "0             1     base         0.2  3  Eucledian  0.948905   0.948736  \\\n",
      "1             1     base         0.2  4  Eucledian  0.948905   0.948736   \n",
      "2             1     base         0.2  5  Eucledian  0.897810   0.899688   \n",
      "3             1     base         0.2  6  Eucledian  0.890511   0.893068   \n",
      "\n",
      "     recall  sensitivity  \n",
      "0  0.948905     0.967033  \n",
      "1  0.948905     0.967033  \n",
      "2  0.897810     0.967033  \n",
      "3  0.890511     0.967033  \n",
      "Empty DataFrame\n",
      "Columns: [dataset_name, knn_type, test_split, k, distance, accuracy, precision, recall, sensitivity]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "parameters_dataset = pd.read_csv('test_parameters.csv')\n",
    "\n",
    "print (\"Here's a glimpse of the dataset generated:\\n \")\n",
    "print(parameters_dataset.iloc[0:4])\n",
    "print(parameters_dataset.iloc[150:155])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Combination\n",
    "Below is a matrix displaying the results on dataset 1. Find the best outcomes for a specific dataset ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Test Split  K  Accuracy   Distance\n",
      "Method                                     \n",
      "base            0.3  4  0.951220  Eucledian\n",
      "bagging         0.3  3  0.965854  Manhattan\n"
     ]
    }
   ],
   "source": [
    "def find_best_result(dataset_id):\n",
    "    \n",
    "    df = parameters_dataset[parameters_dataset['dataset_name'] == dataset_id]\n",
    "    \n",
    "    methods = ['base', 'bagging'] #ADICIONAR FEATURES\n",
    "    best_results = []\n",
    "\n",
    "    for method in methods:\n",
    "        method_df = df[df['knn_type'] == method]\n",
    "        if not method_df.empty:\n",
    "            max_accuracy_index = method_df['accuracy'].idxmax()\n",
    "            best_row = method_df.loc[max_accuracy_index]\n",
    "\n",
    "            best_test_split = best_row['test_split']\n",
    "            best_k = best_row['k']\n",
    "            best_accuracy = best_row['accuracy']\n",
    "            best_distance = best_row['distance']\n",
    "\n",
    "            best_results.append({\n",
    "                'Method': method,\n",
    "                'Test Split': best_test_split,\n",
    "                'K': best_k,\n",
    "                'Accuracy': best_accuracy,\n",
    "                'Distance': best_distance\n",
    "            })\n",
    "        else:\n",
    "            best_results.append({\n",
    "                'Method': method,\n",
    "                'Test Split': None,\n",
    "                'K': None,\n",
    "                'Accuracy': None,\n",
    "                'Distance': None\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(best_results)\n",
    "    results_df.set_index('Method', inplace=True)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Example of usage\n",
    "results_table = find_best_result(1) \n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          dataset_name  test_split  k   distance  accuracy  precision   \n",
      "knn_type                                                                \n",
      "base                 1         0.3  4  Eucledian  0.951220   0.951096  \\\n",
      "bagging              1         0.3  3  Manhattan  0.965854   0.965793   \n",
      "features             0         NaN  0       None       NaN        NaN   \n",
      "\n",
      "            recall  sensitivity  \n",
      "knn_type                         \n",
      "base      0.951220     0.969925  \n",
      "bagging   0.965854     0.977444  \n",
      "features       NaN          NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_best_result(dataset_id):\n",
    "    df = parameters_dataset[parameters_dataset['dataset_name'] == dataset_id]\n",
    "    methods = ['base', 'bagging', 'features']  # Inclui 'features' para depois \n",
    "\n",
    "    best_results = []\n",
    "\n",
    "    for method in methods:\n",
    "        method_df = df[df['knn_type'] == method]\n",
    "        if not method_df.empty:\n",
    "            max_accuracy = method_df['accuracy'].max()\n",
    "            tied_accuracy_df = method_df[method_df['accuracy'] == max_accuracy]\n",
    "\n",
    "            if len(tied_accuracy_df) > 1:\n",
    "                max_precision = tied_accuracy_df['precision'].max()\n",
    "                tied_precision_df = tied_accuracy_df[tied_accuracy_df['precision'] == max_precision]\n",
    "\n",
    "                if len(tied_precision_df) > 1:\n",
    "                    max_recall = tied_precision_df['recall'].max()\n",
    "                    tied_recall_df = tied_precision_df[tied_precision_df['recall'] == max_recall]\n",
    "\n",
    "                    if len(tied_recall_df) > 1:\n",
    "                        max_sensitivity = tied_recall_df['sensitivity'].max()\n",
    "                        tied_sensitivity_df = tied_recall_df[tied_recall_df['sensitivity'] == max_sensitivity]\n",
    "\n",
    "                        if len(tied_sensitivity_df) > 1:\n",
    "                            best_results.extend(tied_sensitivity_df.to_dict('records'))\n",
    "                            continue\n",
    "                    else:\n",
    "                        best_results.append(tied_recall_df.iloc[0].to_dict())\n",
    "                        continue\n",
    "                else:\n",
    "                    best_results.append(tied_precision_df.iloc[0].to_dict())\n",
    "                    continue\n",
    "            else:\n",
    "                best_results.append(tied_accuracy_df.iloc[0].to_dict())\n",
    "                continue\n",
    "        else:\n",
    "            best_results.append({\n",
    "                'knn_type': method,\n",
    "                'dataset_name': None,\n",
    "                'test_split': None,\n",
    "                'k': None,\n",
    "                'distance': None,\n",
    "                'accuracy': None,\n",
    "                'precision': None,\n",
    "                'recall': None,\n",
    "                'sensitivity': None\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(best_results)\n",
    "\n",
    "    # C 'k' e 'dataset_name' para inteiros\n",
    "    if 'k' in results_df.columns and results_df['k'].notna().any():\n",
    "        results_df['k'] = results_df['k'].fillna(0).astype(int)\n",
    "    if 'dataset_name' in results_df.columns and results_df['dataset_name'].notna().any():\n",
    "        results_df['dataset_name'] = results_df['dataset_name'].fillna(0).astype(int)\n",
    "\n",
    "    # Definindo 'knn_type' como √≠ndice\n",
    "    if 'knn_type' in results_df.columns:\n",
    "        results_df.set_index('knn_type', inplace=True)\n",
    "\n",
    "    final_columns = ['dataset_name', 'test_split', 'k', 'distance', 'accuracy', 'precision', 'recall', 'sensitivity']\n",
    "    results_df = results_df[final_columns]  \n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Exemplo de uso\n",
    "results_table = find_best_result(1)\n",
    "print(results_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_knn_comparison(results, kmax=10):\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    test_splits = list(results['KNNBase'].keys())\n",
    "    ks = list(range(1, kmax + 1))\n",
    "    styles = ['-', '--', ':']\n",
    "    colors = ['b', 'g', 'r']\n",
    "    labels = ['Base', 'Bagging', 'Features']\n",
    "\n",
    "    for i, method in enumerate(results):\n",
    "        for j, split in enumerate(test_splits):\n",
    "            ax.plot(ks, results[method][split], label=f'{method} Split={split}', linestyle=styles[j], color=colors[i])\n",
    "\n",
    "    ax.set_xlabel('Number of Neighbors (k)', fontsize=14)\n",
    "    ax.set_ylabel('Accuracy', fontsize=14)\n",
    "    ax.set_title('Comparison of KNN Methods Across Different Test Splits', fontsize=16)\n",
    "    ax.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Statistics\n",
    "[[go back to the topic]](#improved-algorithm)\n",
    "\n",
    "This section provides a suite of Python functions tailored for visual assessment and statistical evaluation of machine learning models, with a particular emphasis on different configurations of the k-Nearest Neighbors (k-NN) algorithm. Each function in this section is crafted to perform specific analyses and visualizations that contribute to a comprehensive understanding of model behavior and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix \n",
    "A confusion matrix is a performance evaluation tool in machine learning, representing the accuracy of a classification model. It displays the number of true positives, true negatives, false positives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_ConfusionMatrix(model_fit,X_test,y_test,y_train,predictions):\n",
    "    y_pred = predictions\n",
    "    unique_classes = np.unique(y_train)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                  display_labels=unique_classes)\n",
    "    disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA\n",
    "A PCA plot illustrates sample similarities in a dataset by correlating initial variables with the first and second principal components (PCs, where PC1 captures the most variation and pc2 the second most). It reduces high-dimensional data, simplify data while preserving trends and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_PCA(X,y):\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_features = pca.fit_transform(X)\n",
    "    \n",
    "    pca_df = pd.DataFrame(data=pca_features, columns=['PC1', 'PC2'])\n",
    "    pca_df['target'] = y\n",
    "    pca_df['target'] = pca_df['target']\n",
    "    \n",
    "    sns.scatterplot(x=pca_df['PC1'],y=pca_df['PC2'],hue=pca_df['target'],palette=\"colorblind\")\n",
    "    plt.title('PCA Graph')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve \n",
    "A ROC curve shows how well a model distinguishes between classes. A steeper curve means better performance, with higher sensitivity and lower false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_roc_curve(clf,X_test,y_test):\n",
    "\n",
    "    #obtain the probabilities (first column)\n",
    "    y_pred_prob = clf.predict_probabilities(X_test)\n",
    "    first_columns = [p[0] for p in y_pred_prob]\n",
    "\n",
    "    #convert no numeric\n",
    "    my_label = preprocessing.LabelEncoder() \n",
    "    y_test_encoded = my_label.fit_transform(y_test)\n",
    "    fpr, tpr,_= roc_curve(y_test_encoded, first_columns)\n",
    "    auc = roc_auc_score(y_test_encoded, first_columns)\n",
    "\n",
    "    #create ROC curve\n",
    "    plt.plot(fpr,tpr, label=\"AUC=\"+str(round(auc,3)))\n",
    "    plt.plot(np.linspace(0,1,5),np.linspace(0,1,5),linestyle='--',linewidth=2)\n",
    "    plt.title('ROC Curve')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Performance\n",
    "[[go back to the topic]](#chosen-algorithm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical performance is evaluated by calculating metrics including accuracy, precision, recall, error rate, sensitivity, and specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9707317073170731\n",
      "Recall: 0.9707317073170731\n",
      "Error Rate: 0.029268292682926855\n",
      "Sensitivity: 0.9774436090225563\n",
      "Specificity: 0.9583333333333334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Precision': 0.9707317073170731,\n",
       " 'Recall': 0.9707317073170731,\n",
       " 'Error Rate': 0.029268292682926855,\n",
       " 'Sensitivity': 0.9774436090225563,\n",
       " 'Specificity': 0.9583333333333334}"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, f1_score, accuracy_score\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "def estatistics(dataset, algorithm, k, test_size):\n",
    "    \n",
    "    if algorithm == 'base':\n",
    "        accuracy, y_test, predictions, X_test, clf, distance = train_base(dataset, k, test_size)\n",
    "    elif algorithm == 'bagging':\n",
    "        accuracy, y_test, predictions, X_test, clf, distance  = train_bagging_sameK(dataset, k, test_size)\n",
    "    else:\n",
    "        accuracy, y_test, predictions, X_test, clf, distance  = train_features(dataset, k, test_size)\n",
    "    \n",
    "    metrics = {}\n",
    "    metrics['Precision'] = precision_score(y_test, predictions, average='weighted', zero_division=1)\n",
    "    metrics['Recall'] = recall_score(y_test, predictions, average='weighted')\n",
    "    metrics['Error Rate'] = 1 - accuracy\n",
    "    \n",
    "    if is_numeric_dtype(y_test) and len(np.unique(y_test)) == 2:\n",
    "        cm1 = confusion_matrix(y_test, predictions)\n",
    "        metrics['Sensitivity'] = cm1[0,0] / (cm1[0,0] + cm1[0,1]) if cm1[0,0] + cm1[0,1] > 0 else 0\n",
    "        metrics['Specificity'] = cm1[1,1] / (cm1[1,0] + cm1[1,1]) if cm1[1,0] + cm1[1,1] > 0 else 0\n",
    "\n",
    "    if not is_numeric_dtype(y_test) or len(np.unique(y_test)) > 2:\n",
    "        metrics['F1-score'] = f1_score(y_test, predictions, average='weighted')\n",
    "        \n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Example usage\n",
    "estatistics(df1, 'bagging', 1, 0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_algorithm_Option(dataset,k,algorithm,option):\n",
    "    if algorithm == 1:\n",
    "        accuracy,y_test,predictions,X_test,clf,X,Y,y_train = train_base(dataset,k)\n",
    "    elif algorithm == 2:\n",
    "        accuracy,y_test,predictions,X_test,clf,X,Y,y_train = train_bagging_sameK(dataset,k)\n",
    "    else:\n",
    "        accuracy,y_test,predictions,X_test,clf,X,Y,y_train = train_features(dataset,k)\n",
    "        \n",
    "    precision = precision_score(y_test, predictions, average='weighted', zero_division=1)\n",
    "    recall = recall_score(y_test, predictions, average='weighted')\n",
    "    er = 1-accuracy\n",
    "    cm1 = confusion_matrix(y_test, predictions)\n",
    "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    specificity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    \n",
    "    if option == 'PCA':\n",
    "        p_PCA(X,Y)\n",
    "    if option == 'ROC':\n",
    "        p_roc_curve(clf,X_test,y_test)\n",
    "    #option == 'Comparison'\n",
    "    else :\n",
    "        find_k_knn_comparison(X,Y,dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#depois ver se √© importante, mas √© para ver se um dataset se ajusta demasiado ou nao \n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
