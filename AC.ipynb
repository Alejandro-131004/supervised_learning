{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "# Supervised Learning\n",
    "\n",
    "Work assembled by Alejandro Gon√ßalves, Pedro Fernandes, Francisca Mihalache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "\n",
    "2. [Chosen Algorithm](#chosen-algorithm)\n",
    "\n",
    "3. [Metrics](#metrics)\n",
    "            \n",
    "4. [Distances](#distances)\n",
    "   - 4.1. [Euclidian Distance](#euclidian-distance)\n",
    "   - 4.2. [L2 Distance](#l2-distance)\n",
    "   - 4.3. [Euclidian Distance (features)](#euclidian-distance-features)\n",
    "   - 4.4. [Cosine Similarity](#cosine-similarity)\n",
    "   - 4.5. [Manhattan Distance](#manhattan-distance)\n",
    "   - 4.6. [Jaccard Distance](#jaccard-distance)\n",
    "   - 4.7. [Mahalanobis Distace](#mahalanobis-distance)\n",
    "5. [Base](#base)\n",
    "\n",
    "6. [Improved Algorithm](#improved-algorithm)\n",
    "   - 6.1. [Bagging](#Bagging)\n",
    "   - 6.2. [KNN-Features](#knn-features)\n",
    "7. [Training](#training)\n",
    "   - 7.1. [Base Training](#base-training)\n",
    "   - 7.2. [Bagging Training](#bagging-training)\n",
    "   - 7.3. [Features Training](#features-training)\n",
    "\n",
    "8. [Comparisons](#comparisons) \n",
    "   - 8.1. [Find k knn](#optimal-number-of-neighborsk)\n",
    "   - 8.2. [Find k Knn comparison](#comparisons-between-models)\n",
    "\n",
    "9. [Statistics](#statistics)\n",
    "   - 9.1 [Confusion Matrix](#confusion-matrix)\n",
    "   - 9.2 [PCA](#pca)\n",
    "   - 9.3 [ROC curve](#roc-curve)\n",
    "   - 9.4 [Statistical Performance](#statistical-performance)\n",
    "\n",
    "10. [Dataset tests](#dataset-tests)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment we'll dive into a selected Machine Learning (ML) algorithm, understanding its theory and testing its performance. We'll explore benchmarking methodologies and differentiate between ML research and practical application, ensuring a balanced understanding of theory and practice in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "To begin with, we need to import some modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score, GridSearchCV, \n",
    "                                     RandomizedSearchCV, cross_validate, LeaveOneOut)\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (classification_report, accuracy_score, precision_score, recall_score,\n",
    "                             confusion_matrix, mean_absolute_error, mean_squared_error, r2_score,\n",
    "                             f1_score, ConfusionMatrixDisplay, roc_curve, roc_auc_score, auc)\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import openml as oml\n",
    "\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/qx/kgqzhwb50b7flqgr05m062t40000gn/T/ipykernel_18469/1163963628.py\", line 1, in <module>\n",
      "    df1 = pd.read_csv('csvs\\\\breast.csv')\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py\", line 912, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py\", line 577, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py\", line 1407, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py\", line 1661, in _make_engine\n",
      "    self.handles = get_handle(\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/pandas/io/common.py\", line 859, in get_handle\n",
      "    handle = open(\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'csvs\\\\breast.csv'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 799, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 845, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/stack_data/core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/pygments/style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"/Users/franciscamihalache/Library/Python/3.9/lib/python/site-packages/pygments/style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('csvs\\\\breast.csv')\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                       0\n",
      "Clump_Thickness          0\n",
      "Cell_Size_Uniformity     0\n",
      "Cell_Shape_Uniformity    0\n",
      "Marginal_Adhesion        0\n",
      "Single_Epi_Cell_Size     0\n",
      "Bare_Nuclei              0\n",
      "Bland_Chromatin          0\n",
      "Normal_Nucleoli          0\n",
      "Mitoses                  0\n",
      "Class                    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# If exists any missing values\n",
    "print(df1.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_1588\\727482383.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df1['Class'] = df1['Class'].replace({'benign': 0, 'malignant': 1})\n",
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_1588\\727482383.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df1['Bare_Nuclei'].replace('?', np.nan, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Clump_Thickness</th>\n",
       "      <th>Cell_Size_Uniformity</th>\n",
       "      <th>Cell_Shape_Uniformity</th>\n",
       "      <th>Marginal_Adhesion</th>\n",
       "      <th>Single_Epi_Cell_Size</th>\n",
       "      <th>Bare_Nuclei</th>\n",
       "      <th>Bland_Chromatin</th>\n",
       "      <th>Normal_Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Clump_Thickness  Cell_Size_Uniformity  Cell_Shape_Uniformity  \\\n",
       "0   1                5                     1                      1   \n",
       "1   2                5                     4                      4   \n",
       "2   3                3                     1                      1   \n",
       "3   4                6                     8                      8   \n",
       "4   5                4                     1                      1   \n",
       "\n",
       "   Marginal_Adhesion  Single_Epi_Cell_Size  Bare_Nuclei  Bland_Chromatin  \\\n",
       "0                  1                     2          1.0                3   \n",
       "1                  5                     7         10.0                3   \n",
       "2                  1                     2          2.0                3   \n",
       "3                  1                     3          4.0                3   \n",
       "4                  3                     2          1.0                3   \n",
       "\n",
       "   Normal_Nucleoli  Mitoses  Class  \n",
       "0                1        1      0  \n",
       "1                2        1      0  \n",
       "2                1        1      0  \n",
       "3                7        1      0  \n",
       "4                1        1      0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replacing benign with 0 and malignant with 1 to be easier to understand and removing the lines that \"?\" is present. \n",
    "df1['Class'] = df1['Class'].replace({'benign': 0, 'malignant': 1})\n",
    "df1['Bare_Nuclei'].replace('?', np.nan, inplace=True)\n",
    "df1['Bare_Nuclei'] = pd.to_numeric(df1['Bare_Nuclei'], errors='coerce')\n",
    "df1.dropna(subset=['Bare_Nuclei'], inplace=True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Clump_Thickness</th>\n",
       "      <th>Cell_Size_Uniformity</th>\n",
       "      <th>Cell_Shape_Uniformity</th>\n",
       "      <th>Marginal_Adhesion</th>\n",
       "      <th>Single_Epi_Cell_Size</th>\n",
       "      <th>Bare_Nuclei</th>\n",
       "      <th>Bland_Chromatin</th>\n",
       "      <th>Normal_Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>352.355783</td>\n",
       "      <td>4.442167</td>\n",
       "      <td>3.150805</td>\n",
       "      <td>3.215227</td>\n",
       "      <td>2.830161</td>\n",
       "      <td>3.234261</td>\n",
       "      <td>3.544656</td>\n",
       "      <td>3.445095</td>\n",
       "      <td>2.869693</td>\n",
       "      <td>1.603221</td>\n",
       "      <td>0.349927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>202.563927</td>\n",
       "      <td>2.820761</td>\n",
       "      <td>3.065145</td>\n",
       "      <td>2.988581</td>\n",
       "      <td>2.864562</td>\n",
       "      <td>2.223085</td>\n",
       "      <td>3.643857</td>\n",
       "      <td>2.449697</td>\n",
       "      <td>3.052666</td>\n",
       "      <td>1.732674</td>\n",
       "      <td>0.477296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>177.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>356.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>527.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>699.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  Clump_Thickness  Cell_Size_Uniformity  \\\n",
       "count  683.000000       683.000000            683.000000   \n",
       "mean   352.355783         4.442167              3.150805   \n",
       "std    202.563927         2.820761              3.065145   \n",
       "min      1.000000         1.000000              1.000000   \n",
       "25%    177.500000         2.000000              1.000000   \n",
       "50%    356.000000         4.000000              1.000000   \n",
       "75%    527.500000         6.000000              5.000000   \n",
       "max    699.000000        10.000000             10.000000   \n",
       "\n",
       "       Cell_Shape_Uniformity  Marginal_Adhesion  Single_Epi_Cell_Size  \\\n",
       "count             683.000000         683.000000            683.000000   \n",
       "mean                3.215227           2.830161              3.234261   \n",
       "std                 2.988581           2.864562              2.223085   \n",
       "min                 1.000000           1.000000              1.000000   \n",
       "25%                 1.000000           1.000000              2.000000   \n",
       "50%                 1.000000           1.000000              2.000000   \n",
       "75%                 5.000000           4.000000              4.000000   \n",
       "max                10.000000          10.000000             10.000000   \n",
       "\n",
       "       Bare_Nuclei  Bland_Chromatin  Normal_Nucleoli     Mitoses       Class  \n",
       "count   683.000000       683.000000       683.000000  683.000000  683.000000  \n",
       "mean      3.544656         3.445095         2.869693    1.603221    0.349927  \n",
       "std       3.643857         2.449697         3.052666    1.732674    0.477296  \n",
       "min       1.000000         1.000000         1.000000    1.000000    0.000000  \n",
       "25%       1.000000         2.000000         1.000000    1.000000    0.000000  \n",
       "50%       1.000000         3.000000         1.000000    1.000000    0.000000  \n",
       "75%       6.000000         5.000000         4.000000    1.000000    1.000000  \n",
       "max      10.000000        10.000000        10.000000   10.000000    1.000000  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols1 = ['Clump_Thickness','Cell_Size_Uniformity']\n",
    "X1 = df1[cols1]\n",
    "y1 = df1['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Clump_Thickness', ylabel='Cell_Size_Uniformity'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(x = X1['Clump_Thickness'], y = X1['Cell_Size_Uniformity'], hue = y1, palette = \"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen Algorithm\n",
    "\n",
    "For this assignment, we decided to use KNN. [[more about K-Nearest Neighbors]](#knnk-nearest-neighbors)\n",
    "\n",
    "In our initial approach, we will implement the algorithm without modifications and analyze the outcomes. Sencondly, we aim to enhance these results by implementing a modified algorithm for comparison. We will adjust the number of k-neighbors, explore different distance methods, and experiment with various features to achieve this improvement.\n",
    "\n",
    "[[go back to the top]](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "[[go back to the topic]](#chosen-algorithm)\n",
    "\n",
    "The metrics.py file contains various performance evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "\n",
    "EPS = 1e-15\n",
    "\n",
    "\n",
    "def unhot(function):\n",
    "    \"\"\"Convert one-hot representation into one column.\"\"\"\n",
    "\n",
    "    def wrapper(actual, predicted):\n",
    "        if len(actual.shape) > 1 and actual.shape[1] > 1:\n",
    "            actual = actual.argmax(axis=1)\n",
    "        if len(predicted.shape) > 1 and predicted.shape[1] > 1:\n",
    "            predicted = predicted.argmax(axis=1)\n",
    "        return function(actual, predicted)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def absolute_error(actual, predicted):\n",
    "    return np.abs(actual - predicted)\n",
    "\n",
    "\n",
    "@unhot\n",
    "def classification_error(actual, predicted):\n",
    "    return (actual != predicted).sum() / float(actual.shape[0])\n",
    "\n",
    "\n",
    "@unhot\n",
    "def accuracy(actual, predicted):\n",
    "    return 1.0 - classification_error(actual, predicted)\n",
    "\n",
    "\n",
    "def mean_absolute_error(actual, predicted):\n",
    "    return np.mean(absolute_error(actual, predicted))\n",
    "\n",
    "\n",
    "def squared_error(actual, predicted):\n",
    "    return (actual - predicted) ** 2\n",
    "\n",
    "\n",
    "def squared_log_error(actual, predicted):\n",
    "    return (np.log(np.array(actual) + 1) - np.log(np.array(predicted) + 1)) ** 2\n",
    "\n",
    "\n",
    "def mean_squared_log_error(actual, predicted):\n",
    "    return np.mean(squared_log_error(actual, predicted))\n",
    "\n",
    "\n",
    "def mean_squared_error(actual, predicted):\n",
    "    return np.mean(squared_error(actual, predicted))\n",
    "\n",
    "\n",
    "def root_mean_squared_error(actual, predicted):\n",
    "    return np.sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "\n",
    "def root_mean_squared_log_error(actual, predicted):\n",
    "    return np.sqrt(mean_squared_log_error(actual, predicted))\n",
    "\n",
    "\n",
    "def logloss(actual, predicted):\n",
    "    predicted = np.clip(predicted, EPS, 1 - EPS)\n",
    "    loss = -np.sum(actual * np.log(predicted))\n",
    "    return loss / float(actual.shape[0])\n",
    "\n",
    "\n",
    "def hinge(actual, predicted):\n",
    "    return np.mean(np.max(1.0 - actual * predicted, 0.0))\n",
    "\n",
    "\n",
    "def binary_crossentropy(actual, predicted):\n",
    "    predicted = np.clip(predicted, EPS, 1 - EPS)\n",
    "    return np.mean(-np.sum(actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted)))\n",
    "\n",
    "\n",
    "# aliases\n",
    "mse = mean_squared_error\n",
    "rmse = root_mean_squared_error\n",
    "mae = mean_absolute_error\n",
    "\n",
    "\n",
    "def get_metric(name):\n",
    "    \"\"\"Return metric function by name\"\"\"\n",
    "    try:\n",
    "        return globals()[name]\n",
    "    except Exception:\n",
    "        raise ValueError(\"Invalid metric function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distances\n",
    "[[go back to the topic]](#chosen-algorithm)\n",
    "\n",
    "The inicial github file provides these functions: \n",
    "\n",
    "- the Euclidean distance \n",
    "\n",
    "- the L2 distance matrix for a set of points in a dataset.\n",
    "  \n",
    "But, as said above, we'll explore different distance methods, like:\n",
    "- Euclidean distance (features)\n",
    "- Cosine Similarity\n",
    "- Manhattan Distance\n",
    "- Jaccard Distance\n",
    "- Mahalanobis distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidian Distance\n",
    "[[go back to the topic]](#distances)\n",
    "\n",
    "This function is designed to calculate the distance between two individual points p1 and p2.\n",
    "Calculates the straight-line distance between two points in Euclidean space, denoted as p1 and p2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "def euclidean_distance(a, b):\n",
    "    if isinstance(a, list) and isinstance(b, list):\n",
    "        a = np.array(a)\n",
    "        b = np.array(b)\n",
    "\n",
    "    return math.sqrt(sum((a - b) ** 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Distance\n",
    "[[go back to the topic]](#distances)\n",
    "\n",
    "\n",
    "The function l2_distance(X) calculates the pairwise L2 or Euclidean distances between rows of a matrix X. This matrix X is expected to be a two-dimensional NumPy array where each row represents a point in a multidimensional space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_distance(X):\n",
    "    sum_X = np.sum(X * X, axis=1)\n",
    "    return (-2 * np.dot(X, X.T) + sum_X).T + sum_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidian Distance (features)\n",
    "[[go back to the topic]](#distances)\n",
    "\n",
    "This function is designed to calculate the Euclidean distances between a single point p1 and multiple points p2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_features(p1, p2):\n",
    "    temp = p1 - p2[:, np.newaxis]\n",
    "    euclid_dist = np.sqrt(np.sum(temp ** 2, axis=-1))\n",
    "    return euclid_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "[[go back to the topic]](#distances)\n",
    "\n",
    "The function cosine_similarity(row1, row2) calculates the cosine similarity between two vectors represented by row1 and row2. Cosine similarity measures the cosine of the angle between two vectors in a multi-dimensional space, providing an indication of how similar the vectors are in terms of orientation, regardless of their magnitude.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(row1, row2):\n",
    "    norm1 = norm(row1)\n",
    "    norm2 = norm(row2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0  # avoid zero division\n",
    "    simi = np.dot(row1, row2) / (norm1 * norm2)\n",
    "    return simi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manhattan Distance\n",
    "[[go back to the topic]](#distances)\n",
    "\n",
    "The function calculates the distance between two vectors a and b.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance(a,b):\n",
    "    return sum(abs(val1-val2) for val1,val2 in zip(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Distance\n",
    "[[go back to the topic]](#distances)\n",
    "\n",
    "The function computes the Jaccard distance between two lists, which represent sets of elements. The Jaccard distance is a measure of dissimilarity between two sets based on the size of their intersection divided by the size of their union.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_distance(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1)+len(list2))-intersection\n",
    "    return float(intersection)/union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mahalanobis distance\n",
    "[[go back to the topic]](#distances)\n",
    "\n",
    "This function computes the Mahalanobis distance that measures the distance between a point and a distribution, considering both the variance and covariance of the distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dist√¢ncia de Mahalanobis: 0.1403242710668961\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def mahalanobis_distance(x, data):\n",
    "    mean = np.mean(data, axis=0)\n",
    "    covariance_matrix = np.cov(data, rowvar=False)\n",
    "    try:\n",
    "        inv_covmat = np.linalg.inv(covariance_matrix)\n",
    "    except np.linalg.LinAlgError:\n",
    "        epsilon = 1e-5\n",
    "        inv_covmat = np.linalg.inv(covariance_matrix + epsilon * np.eye(data.shape[1]))\n",
    "    diff = x - mean\n",
    "\n",
    "    mahalanobis_dist = np.sqrt(np.dot(np.dot(diff.T, inv_covmat), diff))\n",
    "    return mahalanobis_dist\n",
    "\n",
    "#exemplo de teste \n",
    "# Gerar alguns dados aleat√≥rios para teste\n",
    "np.random.seed(42)\n",
    "data = np.random.randn(100, 2) \n",
    "x = np.array([0, 0]) \n",
    "\n",
    "# Calcular a dist√¢ncia de Mahalanobis\n",
    "distance = mahalanobis_distance(x, data)\n",
    "print(\"Dist√¢ncia de Mahalanobis:\", distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base\n",
    "[[go back to the top]](#table-of-contents)\n",
    "\n",
    "Here, we present the implementation of the original algorithm without any modifications.\n",
    "\n",
    "This BaseEstimator python script is used for creating estimators. Includes methods for configuring input data, helping models, and making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BaseEstimator:\n",
    "    y_required = True\n",
    "    fit_required = True\n",
    "\n",
    "    def _setup_input(self, X, y=None):\n",
    "        \"\"\"Ensure inputs to an estimator are in the expected format.\n",
    "\n",
    "        Ensures X and y are stored as numpy ndarrays by converting from an\n",
    "        array-like object if necessary. Enables estimators to define whether\n",
    "        they require a set of y target values or not with y_required, e.g.\n",
    "        kmeans clustering requires no target labels and is fit against only X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like\n",
    "            Feature dataset.\n",
    "        y : array-like\n",
    "            Target values. By default is required, but if y_required = false\n",
    "            then may be omitted.\n",
    "        \"\"\"\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.array(X)\n",
    "\n",
    "        if X.size == 0:\n",
    "            raise ValueError(\"Got an empty matrix.\")\n",
    "\n",
    "        if X.ndim == 1:\n",
    "            self.n_samples, self.n_features = 1, X.shape\n",
    "        else:\n",
    "            self.n_samples, self.n_features = X.shape[0], np.prod(X.shape[1:])\n",
    "\n",
    "        self.X = X\n",
    "\n",
    "        if self.y_required:\n",
    "            if y is None:\n",
    "                raise ValueError(\"Missed required argument y\")\n",
    "\n",
    "            if not isinstance(y, np.ndarray):\n",
    "                y = np.array(y)\n",
    "\n",
    "            if y.size == 0:\n",
    "                raise ValueError(\"The targets array must be no-empty.\")\n",
    "\n",
    "        self.y = y\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._setup_input(X, y)\n",
    "\n",
    "    def predict(self, X=None):\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.array(X)\n",
    "\n",
    "        if self.X is not None or not self.fit_required:\n",
    "            return self._predict(X)\n",
    "        else:\n",
    "            raise ValueError(\"You must call `fit` before `predict`\")\n",
    "\n",
    "    def _predict(self, X=None):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original KNN\n",
    "[[go back to the topic]](#base)\n",
    "\n",
    "This Python code defines a base framework and specific implementations for the k-nearest neighbors (KNN) algorithm, with a classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "\n",
    "class KNNBase(BaseEstimator):\n",
    "    def __init__(self, k=7, distance_func=euclidean_distance):\n",
    "        \"\"\"Base class for Nearest neighbors classifier and regressor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        k : int, default 5\n",
    "            The number of neighbors to take into account. If 0, all the\n",
    "            training examples are used.\n",
    "        distance_func : function, default euclidean distance\n",
    "            A distance function taking two arguments. Any function from\n",
    "            scipy.spatial.distance will do.\n",
    "        \"\"\"\n",
    "\n",
    "        self.k = None if k == 0 else k  # l[:None] returns the whole list\n",
    "        self.distance_func = distance_func\n",
    "\n",
    "    def aggregate(self, neighbors_targets):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _predict(self, X=None):\n",
    "        predictions = [self._predict_x(x) for x in X]\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict_x(self, x):\n",
    "        \"\"\"Predict the label of a single instance x.\"\"\"\n",
    "\n",
    "        # compute distances between x and all examples in the training set.\n",
    "        distances = (self.distance_func(x, example) for example in self.X )\n",
    "\n",
    "        # Sort all examples by their distance to x and keep their target value.\n",
    "        neighbors = sorted(((dist, target) for (dist, target) in zip(distances, self.y)), key=lambda x: x[0])\n",
    "\n",
    "        # Get targets of the k-nn and aggregate them (most common one or\n",
    "        # average).\n",
    "        neighbors_targets = [target for (_, target) in neighbors[: self.k]]\n",
    "\n",
    "        return self.aggregate(neighbors_targets)\n",
    "\n",
    "\n",
    "class KNNClassifier(KNNBase):\n",
    "    \"\"\"Nearest neighbors classifier.\n",
    "\n",
    "    Note: if there is a tie for the most common label among the neighbors, then\n",
    "    the predicted label is arbitrary.\"\"\"\n",
    "\n",
    "    def aggregate(self, neighbors_targets):\n",
    "        \"\"\"Return the most common target label.\"\"\"\n",
    "\n",
    "        most_common_label = Counter(neighbors_targets).most_common(1)[0][0]\n",
    "        return most_common_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "This Python script defines and executes a classification task using a k-nearest neighbors (KNN) algorithm from a synthetic dataset.\n",
    "\n",
    "[[bo back to the topic]](#base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification accuracy 0.96\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "def classification():\n",
    "    X, y = make_classification(\n",
    "        n_samples=500,\n",
    "        n_features=5,\n",
    "        n_informative=5,\n",
    "        n_redundant=0,\n",
    "        n_repeated=0,\n",
    "        n_classes=2,\n",
    "        random_state=1111,\n",
    "        class_sep=1.5,\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1111)\n",
    "\n",
    "    clf = KNNClassifier(k=7, distance_func=euclidean_distance)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    print(\"classification accuracy\", accuracy(y_test, predictions))\n",
    "    #print(classification_report(y_test, predictions))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Algorithm\n",
    "[[go back to the top]](#table-of-contents)\n",
    "\n",
    "In this chapter we will approach bagging [[more about bagging]](#bagging)\n",
    "\n",
    "In machine learning, blending different methods often boosts resilience. By merging KNN Bagging and KNN Features, we harness ensemble learning and feature customization for more robust models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging \n",
    "[[go back to the topic]](#improved-algorithm)\n",
    "\n",
    "**Bagging**, short for Bootstrap Aggregating is an ensemble technique that improves model stability and accuracy by training multiple models on different subsets of data and combining their predictions. This method reduces variance, enhances performance, and minimizes overfitting.\n",
    "\n",
    "\n",
    "The `KNNBagging` class integrates the principles of bagging with the k-Nearest Neighbors (k-NN) algorithm. By allowing the selection of various distance metrics, it enhances the adaptability and precision of the k-NN method:\n",
    "\n",
    "After calculating distances using various metrics, the k nearest neighbors are identified for each test instance and the most common label among them are selected. This technique aggregates outcomes from different sub-models, effectively reducing prediction variance and improving accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classe que eu acho que esta funcional \n",
    "class KNN_Bagging:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def _get_distances(self, x, distance_func):\n",
    "        return [distance_func(x, x_train) for x_train in self.X_train]\n",
    "\n",
    "    def _get_labels(self, distances):\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        return [self.y_train[idx] for idx in k_indices]\n",
    "\n",
    "    def _most_frequent_label(self, labels):\n",
    "        unique_classes, counts = np.unique(labels, return_counts=True)\n",
    "        return unique_classes[np.argmax(counts)]\n",
    "\n",
    "    def predict(self, X, metric_index):\n",
    "        distance_functions = [euclidean_distance, cosine_similarity, manhattan_distance, jaccard_distance]\n",
    "        predictions = []\n",
    "\n",
    "        for x in X:\n",
    "            distances = self._get_distances(x, distance_functions[metric_index])\n",
    "            labels = self._get_labels(distances)\n",
    "            predictions.append(self._most_frequent_label(labels))\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    def predict_probabilities(self, X):\n",
    "        y_proba = []\n",
    "        for x in X:\n",
    "            distances = np.sqrt(np.sum((self.X_train - x) ** 2, axis=1))\n",
    "            sorted_indices = np.argsort(distances)[:self.k]\n",
    "            labels = self.y_train[sorted_indices]\n",
    "            unique_classes, counts = np.unique(labels, return_counts=True)\n",
    "            class_frequencies = counts / self.k\n",
    "            y_proba.append(class_frequencies)\n",
    "\n",
    "        return np.array(y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN-Features\n",
    "[[go back to the topic]](#improved-algorithm)\n",
    "\n",
    "The `KNN_Features` class optimizes the k-Nearest Neighbors algorithm by prioritizing influential features in determining data point proximity. It calculates distances with a chosen metric, identifies the k nearest neighbors, and selects the most prevalent label among them, boosting prediction accuracy. Furthermore, it estimates class probabilities based on nearest neighbor label frequencies, offering a quantifiable prediction confidence measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_Features:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return  [self._predict(x) for x in X]\n",
    "    \n",
    "    def _predict(self, x):\n",
    "        distances = [euclidean_distance_features(x, x_train) for x_train in self.X_train]\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = self.y_train[k_indices]\n",
    "        most_frequent_label = np.bincount(k_nearest_labels).argmax()\n",
    "        return most_frequent_label\n",
    "\n",
    "    def predict_probabilities(self, X):\n",
    "        y_proba = []\n",
    "        for x in X:\n",
    "            distances = np.linalg.norm(self.X_train - x, axis=1)\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            k_nearest_classes = self.y_train[k_indices]\n",
    "            class_frequencies = np.bincount(k_nearest_classes, minlength=len(np.unique(self.y_train))) / self.k\n",
    "            y_proba.append(class_frequencies)\n",
    "        return np.array(y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KNN_Features:\n",
    "    def __init__(self, num_neighbors):\n",
    "        self.num_neighbors = num_neighbors\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def single_prediction(self, test_instance):\n",
    "        distances = np.sqrt(((self.features_train - test_instance) ** 2).sum(axis=1))\n",
    "        nearest_neighbors_indices = np.argsort(distances)[:self.num_neighbors]\n",
    "        nearest_labels = self.labels_train[nearest_neighbors_indices]\n",
    "        return np.bincount(nearest_labels).argmax()\n",
    "\n",
    "    def predict(self, features_test):\n",
    "        return np.array([self.single_prediction(x) for x in features_test])\n",
    "\n",
    "    def predict_probabilities(self, features_test):\n",
    "        probabilities = []\n",
    "        for x in features_test:\n",
    "            distances = np.sqrt(((self.features_train - x) ** 2).sum(axis=1))\n",
    "            nearest_neighbors_indices = np.argsort(distances)[:self.num_neighbors]\n",
    "            nearest_labels = self.labels_train[nearest_neighbors_indices]\n",
    "            label_freq = np.bincount(nearest_labels, minlength=len(np.unique(self.labels_train))) / self.num_neighbors\n",
    "            probabilities.append(label_freq)\n",
    "        return np.array(probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Training is a critical process in machine learning where a model learns from a dataset to identify patterns and make informed decisions. This process is essential as it equips the model to accurately predict outcomes on new, unseen data.\n",
    "\n",
    "We will implement three distinct training approaches to optimize our model's performance:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base training\n",
    "[[go back to the topic]](#chosen-algorithm)\n",
    "\n",
    "Tests on the Original k-NN Implementation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Base(dataset, k, test_size):\n",
    "    random_state = 1234\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(target=dataset.default_target_attribute)\n",
    "    df = pd.DataFrame(X, columns=attribute_names)\n",
    "    numeric_cols = [c for c in df.columns if is_numeric_dtype(df[c])]\n",
    "    X = df[numeric_cols].values\n",
    "    Y = y.to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state)\n",
    "    clf = KNNClassifier(k=k)  # Assume KNNClassifier j√° est√° definido\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return accuracy, y_test, predictions, X_test, clf, X, Y, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Training\n",
    "[[go back to the topic]](#improved-algorithm)\n",
    "\n",
    "This function employs a KNN_Bagging classifier trained on various distance metrics (Euclidean, Cosine, Manhattan, Jaccard, and Mahalanobis). For each metric, it generates predictions and aggregates the most common results across all metrics to enhance prediction robustness. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bagging(dataset, k, test_size):\n",
    "    random_state = 1234\n",
    "    X, y, _, attribute_names = dataset.get_data(target=dataset.default_target_attribute)\n",
    "    df = pd.DataFrame(X, columns=attribute_names)\n",
    "    numeric_cols = df.select_dtypes(include='number').columns.tolist()\n",
    "    X_numeric = df[numeric_cols].values\n",
    "    y_array = y.to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_numeric, y_array, test_size=test_size, random_state=random_state)\n",
    "    clf = KNN_Bagging(k)  \n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions_general = []\n",
    "    for idx in range(5): \n",
    "        predictions = clf.predict(X_test, idx)\n",
    "        most_common = Counter(predictions).most_common(1)[0][0]\n",
    "        predictions_general.append(most_common)\n",
    "    accuracy = accuracy_score(y_test, predictions_general)\n",
    "    return accuracy, y_test, predictions_general, X_test, clf, X, y, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-NN Feature Ensemble\n",
    "This function constructs an ensemble of k-Nearest Neighbors (k-NN) classifiers, each trained on a **randomly selected subset of features from the data**. This approach diversifies the training process by using different feature combinations for each classifier, enhancing accuracy of the ensemble. The classifiers are then compiled, and the most effective one, determined by frequency of selection, is chosen to represent the ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MUDAR ESTA FUNCAOOOO\n",
    "def knn_ensemble(X, y, n_neighbors, n_classifiers, sample_size):\n",
    "    \n",
    "    #subconjuntos de features aleat√≥rias para cada classificador\n",
    "    n_samples, n_features = X.shape\n",
    "    feature_subsets = [np.random.choice(range(n_features), int(sample_size * n_features), replace=False) for _ in range(n_classifiers)]\n",
    "    classifiers = []\n",
    "\n",
    "    # creat a trainning sample with resampling\n",
    "    for features in feature_subsets:\n",
    "        X_subset = X[:, features]\n",
    "        X_resampled, y_resampled = resample(X_subset, y, n_samples=int(sample_size * n_samples), replace=True, random_state=42)\n",
    "        cls = KNN_Features(5)\n",
    "        cls.fit(X_resampled, y_resampled)\n",
    "        classifiers.append(cls)\n",
    "    \n",
    "    #Ensemble of classifiers\n",
    "    ensemble = Counter(classifiers).most_common(1)[0][0]\n",
    "    \n",
    "    return ensemble,cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Training \n",
    "\n",
    "Train an ensemble of k-NN models using `knn_ensemble` on the training set, where each model employs a unique feature subset to enhance generalization and reduce overfitting. Then, use this ensemble to predict outcomes on the test set and assess its performance by calculating accuracy in capturing dataset patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_features(dataset, k, test_size):\n",
    "    random_state = 1234\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(target=dataset.default_target_attribute)\n",
    "    df = pd.DataFrame(X, columns=attribute_names)\n",
    "    numeric_cols = [c for c in df.columns if is_numeric_dtype(df[c])]\n",
    "    X = df[numeric_cols].values\n",
    "    Y = y.to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    ensemble, clf = knn_ensemble(X, y, k, 5, 0.8)\n",
    "    predictions = ensemble.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return accuracy,y_test,predictions,X_test,clf,X,Y,y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal number of Neighbors(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve reliable predictions and avoid overfitting and underfitting it¬¥s important to find the best K neighbors for a certain dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_knn(X,y,algorithm,kmax=10):\n",
    "    k_values = [i for i in range (1,kmax+1)]\n",
    "    accuracies = list()\n",
    "    for k in k_values:\n",
    "        if algorithm == 1:\n",
    "            knn = KNNClassifier(k)\n",
    "            accuracy,y_test,predictions,X_test,clf,X,Y,y_train = train_Base(dataset,k)\n",
    "        elif algorithm == 2:\n",
    "            knn = KNN_Bagging(k)\n",
    "            accuracy,y_test,predictions,X_test,clf,X,Y,y_train = train_bagging(dataset,k)\n",
    "        else:\n",
    "            knn = KNN_Features(k)\n",
    "            accuracy,y_test,predictions,X_test,clf,X,Y,y_train = train_features(dataset,k)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    #Ploting the results of Cross Validation\n",
    "    sns.lineplot(x = k_values, y = accuracies, marker = 'o')\n",
    "    plt.xlabel(\"K Values\")\n",
    "    plt.ylabel(\"Accuracy Score of k-NN\")\n",
    "    plt.title('Estimates by Cross Validation')\n",
    "    \n",
    "    best_index = np.argmax(accuracies)    \n",
    "    best_k = k_values[best_index]    \n",
    "    print(f'Best k = {best_k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons (between models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of different k-NN models is compared by evaluating their accuracy scores across various values of k. It plots the results of cross-validation and identifies the best k value for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_knn_comparison(X,y,kmax=10):\n",
    "    k_values = [i for i in range (1,kmax+1)]\n",
    "    accuracies_base = list()\n",
    "    accuracies_bagging = list()\n",
    "    accuracies_features = list()\n",
    "    for k in k_values:\n",
    "        knn_base = KNNBase(k)\n",
    "        knn_bagging = KNN_Bagging(k)\n",
    "        knn_features = KNN_Features(k)\n",
    "        \n",
    "        accuracy_base,y_test,predictions,X_test,clf,X,Y,y_train = train_Base(dataset,k)\n",
    "        accuracy_bagging,y_test,predictions,X_test,clf,X,Y,y_train = train_bagging(dataset,k)\n",
    "        accuracy_features,y_test,predictons,X_test,clf,X,Y,y_train = train_features(dataset,k)\n",
    "        \n",
    "        accuracies_base.append(accuracy_base)\n",
    "        accuracies_bagging.append(accuracy_bagging)\n",
    "        accuracies_features.append(accuracy_features)\n",
    "\n",
    "    #Ploting the results of Cross Validation\n",
    "    sns.lineplot(x = k_values, y = accuracies_base, marker = 'o')\n",
    "    sns.lineplot(x = k_values, y = accuracies_bagging, marker = 'o')\n",
    "    sns.lineplot(x = k_values, y = accuracies_features, marker = 'o')\n",
    "    plt.xlabel(\"K Values\")\n",
    "    plt.ylabel(\"Accuracy Score of k-NN\")\n",
    "    plt.title('Estimates by Cross Validation')\n",
    "    \n",
    "    best_index_base = np.argmax(accuracies_base)    \n",
    "    best_k_base = k_values[best_index_base] \n",
    "    best_index_bagging = np.argmax(accuracies_bagging)    \n",
    "    best_k_bagging = k_values[best_index_bagging] \n",
    "    best_index_features = np.argmax(accuracies_features)    \n",
    "    best_k_features = k_values[best_index_features] \n",
    "    print(f'Best k algorithm Base = {best_k_base}')\n",
    "    print(f'Best k Bagging = {best_k_bagging}')\n",
    "    print(f'Best k Features = {best_k_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_k_and_test_split(dataset, knn_type, kmax=10, test_splits=[0.2, 0.3, 0.4]):\n",
    "    best_accuracy = 0\n",
    "    best_k = None\n",
    "    best_test_split = None\n",
    "    results = {}  # Dicion√°rio para armazenar os resultados para depois plotar \n",
    "\n",
    "    for test_split in test_splits:\n",
    "        results[test_split] = {}\n",
    "        for k in range(1, kmax + 1):\n",
    "            if knn_type == 'base':\n",
    "                accuracy, y_test, predictions, X_test, clf, X, Y, y_train = train_Base(dataset, k, test_split)\n",
    "            elif knn_type == 'bagging':\n",
    "                accuracy, y_test, predictions, X_test, clf, X, Y, y_train = train_bagging(dataset, k, test_split)\n",
    "            elif knn_type == 'features':\n",
    "                accuracy, y_test, predictions, X_test, clf, X, Y, y_train = train_features(dataset, k, test_split)\n",
    "\n",
    "            results[test_split][k] = accuracy\n",
    "\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_k = k\n",
    "                best_test_split = test_split\n",
    "\n",
    "            #print(f\"Tested: Type={knn_type}, Split={test_split}, k={k}, Accuracy={accuracy}\")\n",
    "\n",
    "    #print(f\"Best setup for {knn_type}: Test Split={best_test_split}, k={best_k} with Accuracy={best_accuracy}\")\n",
    "    return best_test_split, best_k, best_accuracy, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_best_k_test_split(results):\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    markers = ['o', 's', '^']  # Diferentes marcadores para cada test_split\n",
    "    colors = ['b', 'g', 'r']   # Cores diferentes para cada test_split\n",
    "    for i, (test_split, accuracies) in enumerate(results.items()):\n",
    "        ks = list(accuracies.keys())\n",
    "        accuracy_vals = list(accuracies.values())\n",
    "        ax.plot(ks, accuracy_vals, label=f'Test Split = {test_split}', marker=markers[i], color=colors[i])\n",
    "\n",
    "    ax.set_xlabel('Number of Neighbors (k)', fontsize=14)\n",
    "    ax.set_ylabel('Accuracy', fontsize=14)\n",
    "    ax.set_title('KNN Validation Results by k and Test Split', fontsize=16)\n",
    "    ax.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_knn_methods(dataset, kmax=10, test_splits=[0.2, 0.3, 0.4]):\n",
    "    results = {'KNNBase': {}, 'KNNBagging': {}, 'KNNFeatures': {}}\n",
    "\n",
    "    for test_split in test_splits:\n",
    "        X, y = dataset\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_split, random_state=42)\n",
    "\n",
    "        for k in range(1, kmax + 1):\n",
    "            # KNNBase\n",
    "            knn_base = KNNBase(k)\n",
    "            knn_base.fit(X_train, y_train)\n",
    "            predictions_base = knn_base.predict(X_test)\n",
    "            accuracy_base = accuracy_score(y_test, predictions_base)\n",
    "            results['KNNBase'].setdefault(test_split, []).append(accuracy_base)\n",
    "\n",
    "            # KNNBagging\n",
    "            knn_bagging = KNN_Bagging(k)\n",
    "            knn_bagging.fit(X_train, y_train)\n",
    "            predictions_bagging = knn_bagging.predict(X_test)\n",
    "            accuracy_bagging = accuracy_score(y_test, predictions_bagging)\n",
    "            results['KNNBagging'].setdefault(test_split, []).append(accuracy_bagging)\n",
    "\n",
    "            # KNNFeatures\n",
    "            knn_features = KNN_Features(k)\n",
    "            knn_features.fit(X_train, y_train)\n",
    "            predictions_features = knn_features.predict(X_test)\n",
    "            accuracy_features = accuracy_score(y_test, predictions_features)\n",
    "            results['KNNFeatures'].setdefault(test_split, []).append(accuracy_features)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_knn_comparison(results, kmax=10):\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    test_splits = list(results['KNNBase'].keys())\n",
    "    ks = list(range(1, kmax + 1))\n",
    "    styles = ['-', '--', ':']\n",
    "    colors = ['b', 'g', 'r']\n",
    "    labels = ['Base', 'Bagging', 'Features']\n",
    "\n",
    "    for i, method in enumerate(results):\n",
    "        for j, split in enumerate(test_splits):\n",
    "            ax.plot(ks, results[method][split], label=f'{method} Split={split}', linestyle=styles[j], color=colors[i])\n",
    "\n",
    "    ax.set_xlabel('Number of Neighbors (k)', fontsize=14)\n",
    "    ax.set_ylabel('Accuracy', fontsize=14)\n",
    "    ax.set_title('Comparison of KNN Methods Across Different Test Splits', fontsize=16)\n",
    "    ax.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Statistics\n",
    "[[go back to the topic]](#improved-algorithm)\n",
    "\n",
    "This section provides a suite of Python functions tailored for visual assessment and statistical evaluation of machine learning models, with a particular emphasis on different configurations of the k-Nearest Neighbors (k-NN) algorithm. Each function in this section is crafted to perform specific analyses and visualizations that contribute to a comprehensive understanding of model behavior and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix \n",
    "A confusion matrix is a performance evaluation tool in machine learning, representing the accuracy of a classification model. It displays the number of true positives, true negatives, false positives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_ConfusionMatrix(model_fit,X_test,y_test,y_train,predictions):\n",
    "    y_pred = predictions\n",
    "    unique_classes = np.unique(y_train)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                  display_labels=unique_classes)\n",
    "    disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA\n",
    "A PCA plot illustrates sample similarities in a dataset by correlating initial variables with the first and second principal components (PCs, where PC1 captures the most variation and pc2 the second most). It reduces high-dimensional data, simplify data while preserving trends and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_PCA(X,y):\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_features = pca.fit_transform(X)\n",
    "    \n",
    "    pca_df = pd.DataFrame(data=pca_features, columns=['PC1', 'PC2'])\n",
    "    pca_df['target'] = y\n",
    "    pca_df['target'] = pca_df['target']\n",
    "    \n",
    "    sns.scatterplot(x=pca_df['PC1'],y=pca_df['PC2'],hue=pca_df['target'],palette=\"colorblind\")\n",
    "    plt.title('PCA Graph')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve \n",
    "A ROC curve shows how well a model distinguishes between classes. A steeper curve means better performance, with higher sensitivity and lower false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_roc_curve(clf,X_test,y_test):\n",
    "\n",
    "    #obtain the probabilities (first column)\n",
    "    y_pred_prob = clf.predict_probabilities(X_test)\n",
    "    first_columns = [p[0] for p in y_pred_prob]\n",
    "\n",
    "    #convert no numeric\n",
    "    my_label = preprocessing.LabelEncoder() \n",
    "    y_test_encoded = my_label.fit_transform(y_test)\n",
    "    fpr, tpr,_= roc_curve(y_test_encoded, first_columns)\n",
    "    auc = roc_auc_score(y_test_encoded, first_columns)\n",
    "\n",
    "    #create ROC curve\n",
    "    plt.plot(fpr,tpr, label=\"AUC=\"+str(round(auc,3)))\n",
    "    plt.plot(np.linspace(0,1,5),np.linspace(0,1,5),linestyle='--',linewidth=2)\n",
    "    plt.title('ROC Curve')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Performance\n",
    "[[go back to the topic]](#chosen-algorithm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical performance is evaluated by calculating metrics including accuracy, precision, recall, error rate, sensitivity, and specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estatistics(dataset,k,algorithm):\n",
    "    if algorithm == 'base':\n",
    "        accuracy,y_test,predictions,X_test,clf,X,Y,y_train = train_Base(dataset,k)\n",
    "    elif algorithm == 'bagging':\n",
    "        accuracy,y_test,predictions,X_test,clf,X,Y,y_train = train_bagging(dataset,k)\n",
    "    else:\n",
    "        accuracy,y_test,predictions,X_test,clf,X,Y,y_train = train_features(dataset,k)\n",
    "        \n",
    "    precision = precision_score(y_test, predictions, average='weighted', zero_division=1)\n",
    "    recall = recall_score(y_test, predictions, average='weighted')\n",
    "    er = 1-accuracy\n",
    "    cm1 = confusion_matrix(y_test, predictions)\n",
    "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    specificity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    \n",
    "    if not is_numeric_dtype(y_test):\n",
    "        f1 = f1_score(y_test, predictions, average='weighted')\n",
    "        print(\"F1-score:\", f1)\n",
    "        \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"Error Rate:\",er)\n",
    "    print('Sensitivity:', sensitivity)\n",
    "    print('Specificity:', specificity)\n",
    "    find_k_knn(X,Y,algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_algorithm_Option(dataset,k,algorithm,option):\n",
    "    if algorithm == 1:\n",
    "        accuracy,y_test,predictions,X_test,clf,X,Y,y_train = train_Base(dataset,k)\n",
    "    elif algorithm == 2:\n",
    "        accuracy,y_test,predictions,X_test,clf,X,Y,y_train = train_bagging(dataset,k)\n",
    "    else:\n",
    "        accuracy,y_test,predictions,X_test,clf,X,Y,y_train = train_features(dataset,k)\n",
    "        \n",
    "    precision = precision_score(y_test, predictions, average='weighted', zero_division=1)\n",
    "    recall = recall_score(y_test, predictions, average='weighted')\n",
    "    er = 1-accuracy\n",
    "    cm1 = confusion_matrix(y_test, predictions)\n",
    "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    specificity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    \n",
    "    if option == 'PCA':\n",
    "        p_PCA(X,Y)\n",
    "    if option == 'ROC':\n",
    "        p_roc_curve(clf,X_test,y_test)\n",
    "    #option == 'Comparison'\n",
    "    else :\n",
    "        find_k_knn_comparison(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#depois ver se √© importante, mas √© para ver se um dataset se ajusta demasiado ou nao \n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentagem Recomendada para Teste: 20.00%\n",
      "Percentagem Recomendada para Teste: 10.00%\n"
     ]
    }
   ],
   "source": [
    "#Determina a porcentagem de teste adequada com base no n√∫mero de linhas do dataset.\n",
    "\n",
    "def determine_test_percentage(data, min_test_ratio=0.1, max_test_ratio=0.3, threshold=10000):\n",
    "\n",
    "    # Determina o n√∫mero de amostras a partir da primeira dimens√£o do dataset\n",
    "    num_samples = data.shape[0]\n",
    "    if num_samples > threshold:\n",
    "        test_percentage = min_test_ratio\n",
    "    else:\n",
    "        test_percentage = max_test_ratio - ((num_samples - 1) / (threshold - 1)) * (max_test_ratio - min_test_ratio)\n",
    "    \n",
    "    return test_percentage\n",
    "'''\n",
    "# Exemplo de uso com um DataFrame do pandas\n",
    "df = pd.DataFrame(np.random.rand(5000, 10))  # Cria um DataFrame com 5000 linhas e 10 colunas\n",
    "test_percentage = determine_test_percentage(df)\n",
    "print(f'Percentagem Recomendada para Teste: {test_percentage * 100:.2f}%')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best test split percentage: 20%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def best_test_split(data, labels, splits=[0.2, 0.3, 0.4], random_state=42):\n",
    "    best_accuracy = 0\n",
    "    best_split = None\n",
    "\n",
    "    for test_size in splits:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=test_size, random_state=random_state)\n",
    "        model = KNN_Bagging(k=5)  # You can adjust the number of neighbors\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        predictions = model.predict(X_test)\n",
    "        current_accuracy = accuracy_score(y_test, predictions)\n",
    "        \n",
    "        \n",
    "        if current_accuracy > best_accuracy:\n",
    "            best_accuracy = current_accuracy\n",
    "            best_split = test_size\n",
    "    \n",
    "    return best_split\n",
    "\n",
    "# Example usage with synthetic data\n",
    "np.random.seed(42)\n",
    "data = np.random.rand(1000, 10)  # 1000 samples, 10 features\n",
    "labels = np.random.randint(0, 2, 1000)  # 1000 binary labels\n",
    "\n",
    "# Find the best split\n",
    "optimal_test_split = best_test_split(data, labels)\n",
    "print(f'Best test split percentage: {optimal_test_split * 100:.0f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_k_and_test_split(dataset,knn_type, kmax=10, test_splits=[0.2, 0.3, 0.4]):\n",
    "    best_accuracy = 0\n",
    "    best_k = None\n",
    "    best_test_split = None\n",
    "\n",
    "    for test_split in test_splits:\n",
    "        for k in range(1, kmax + 1):\n",
    "            if knn_type == 'base':\n",
    "                accuracy,y_test,predictions,X_test,clf,X,Y,y_train = train_Base(dataset, k, test_split)\n",
    "            elif knn_type == 'bagging':\n",
    "                accuracy,y_test,predictions,X_test,clf,X,Y,y_train= train_bagging(dataset, k, test_split)\n",
    "            elif knn_type == 'features':\n",
    "                accuracy,y_test,predictions,X_test,clf,X,Y,y_train = train_features(dataset, k, test_split)\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_k = k\n",
    "                best_test_split = test_split\n",
    "\n",
    "            print(f\"Tested: Type={knn_type}, Split={test_split}, k={k}, Accuracy={accuracy}\")\n",
    "\n",
    "    print(f\"Best setup for {knn_type}: Test Split={best_test_split}, k={best_k} with Accuracy={best_accuracy}\")\n",
    "    return best_test_split, best_k, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
